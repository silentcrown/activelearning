{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/lil.py:19: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _csparsetools\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:165: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._shortest_path import shortest_path, floyd_warshall, dijkstra,\\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._tools import csgraph_to_dense, csgraph_from_dense,\\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:167: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._traversal import breadth_first_order, depth_first_order, \\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:169: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._min_spanning_tree import minimum_spanning_tree\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:170: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._reordering import reverse_cuthill_mckee, maximum_bipartite_matching, \\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/special/__init__.py:640: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ufuncs import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/linalg/basic.py:17: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._solve_toeplitz import levinson\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/linalg/__init__.py:207: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._decomp_update import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/special/_ellip_harm.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/interpolate/_bsplines.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _bspl\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/__init__.py:95: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .ckdtree import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/__init__.py:96: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .qhull import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/_spherical_voronoi.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _voronoi\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/distance.py:122: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _hausdorff\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/optimize/_trlib/__init__.py:1: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._trlib import TRLIBQuadraticSubproblem\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/optimize/_numdiff.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._group_columns import group_dense, group_sparse\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/stats/_continuous_distns.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _stats\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/__init__.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/__init__.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/_hl/group.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .. import h5g, h5i, h5o, h5r, h5t, h5l, h5p\n",
      "Using TensorFlow backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.263 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/utils/__init__.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .murmurhash import murmurhash3_32\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/utils/extmath.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._logistic_sigmoid import _log_logistic_sigmoid\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/utils/extmath.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .sparsefuncs_fast import csr_row_norms\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/metrics/cluster/supervised.py:23: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .expected_mutual_info_fast import expected_mutual_information\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 1006 user-define jieba dict success!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from intent_classify import IntentClassify\n",
    "from gensim import corpora, models, similarities,matutils\n",
    "from fasttext_util import FasttextClassifier\n",
    "import time\n",
    "from collections import Counter\n",
    "import re\n",
    "from url import Searcher\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word():    \n",
    "    def __init__(self):\n",
    "        self.word_to_vec = {}\n",
    "        self.dictionary = corpora.dictionary.Dictionary()\n",
    "        self.dic = self.dictionary.load('../util/new/laiye/corpus.dict')\n",
    "        self.tfidf = models.TfidfModel.load('../util/new/laiye/corpus.tfidf_model')  \n",
    "        \n",
    "    def remove_punctuation(self, line):\n",
    "        rule = re.compile(ur\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")\n",
    "        line = rule.sub('',line)\n",
    "        return line        \n",
    "\n",
    "    def stopwordslist(self, filepath):  \n",
    "        stopwords = [line.strip() for line in open(filepath, 'r').readlines()]  \n",
    "        return stopwords  \n",
    "\n",
    "\n",
    "    # 对句子进行分词  \n",
    "    def jieba_cut(self, sentence):  \n",
    "        sentence_seged = jieba.cut(sentence.strip())  \n",
    "        stopwords = stopwordslist('../util/new/laiye/stopwords.txt')  # 这里加载停用词的路径  \n",
    "        outstr = ''  \n",
    "        for word in sentence_seged:  \n",
    "            if word.encode('utf-8') not in stopwords:  \n",
    "                if word.encode('utf-8') != '\\t' and word != '\\t':  \n",
    "                    outstr += word\n",
    "                    outstr += \" \"  \n",
    "        return outstr.strip().split(' ')\n",
    "\n",
    "    def get_word2vec(self):\n",
    "        with open('../util/new/laiye/w2v_sgns_win1_d80.kv') as f:\n",
    "            data = [x.split(' ') for x in f.readlines()[1:]]\n",
    "            words = [d[0] for d in data]\n",
    "            vecs = np.array([d[1: -1] for d in data], dtype= 'float64')\n",
    "            for i in range(len(words)):\n",
    "                word = words[i]\n",
    "                self.word_to_vec[word] = vecs[i]\n",
    "\n",
    "    def get_tf_idf_of_query(self, query, dic, tfidf):      \n",
    "        vec_bow = dic.doc2bow(query)\n",
    "        vec_tfidf = tfidf[vec_bow]\n",
    "        tp = [0.0] * len(query)\n",
    "        ids = [tid[0] for tid in vec_tfidf]\n",
    "        flags = [0] * len(query)\n",
    "        count = 0.00001\n",
    "        for j in range(len(query)):\n",
    "            for i in range(len(ids)):\n",
    "                if self.dic[ids[i]] == query[j]:\n",
    "                    tp[j] = vec_tfidf[i][1]\n",
    "                    flags[j] = 1\n",
    "                    count += 1\n",
    "                    break\n",
    "        sums = sum(tp)\n",
    "        #print ','.join(query)\n",
    "        for i in range(len(flags)):\n",
    "            if flags[i] == 1:\n",
    "                continue\n",
    "            tp[i] = 1.0 * sums / count\n",
    "                    \n",
    "        # apply l1-norm to tfidf value\n",
    "        #tfidf = matutils.unitvec(vec_tfidf, norm = 'l1')   \n",
    "        return tp\n",
    "\n",
    "    def get_vectors_of_data_cut(self, data):\n",
    "        vecs = []\n",
    "        for i in range(len(data)):\n",
    "            vec = []\n",
    "            d_line = data[i].split(' ')\n",
    "            tfidfs = self.get_tf_idf_of_query(d_line, self.dic, self.tfidf)\n",
    "            s = sum(tfidfs)\n",
    "            for j in range(len(d_line)):\n",
    "                if d_line[j].encode('utf-8') in self.word_to_vec:\n",
    "                    if s == 0:\n",
    "                        vec.append([0.0] * len(self.word_to_vec['家']))\n",
    "                    else:\n",
    "                        vec.append((tfidfs[j] / s) * self.word_to_vec[d_line[j].encode('utf-8')])\n",
    "            if len(vec) == 0:\n",
    "                vec.append([0.0] * len(self.word_to_vec['家']))\n",
    "            vecs.append(np.sum(np.array(vec), axis = 0))\n",
    "        return np.array(vecs)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data():\n",
    "    \n",
    "    def __init__(self, data_path, text_path, knowledge_path, label_path):\n",
    "        self.data = open(data_path, 'r').readlines()\n",
    "        self.knowledge = open(knowledge_path, 'r').readlines()\n",
    "        self.text_path = text_path\n",
    "        self.label = open(label_path, 'r').readlines()\n",
    "        self.indexes = []\n",
    "        self.labeled_indexes = []\n",
    "        self.unlabeled_indexes = []\n",
    "        self.test_indexes = []\n",
    "        self.questions_match_dict = []\n",
    "        self.range = [0.000001,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0]\n",
    "        self.range_result = {0.05:0, 0.1:0,0.15:0, 0.2:0,0.25:0, 0.3:0,0.35:0, 0.4:0,0.45:0,0.5:0, 0.55:0,0.6:0,0.65:0,0.7:0, 0.75:0,0.8:0,0.85:0,0.9:0,0.95:0, 1.0:0}\n",
    "        self.dic = {}\n",
    "        \n",
    "    def get_texts(self, text_path):\n",
    "        texts = []\n",
    "        knowledges = []\n",
    "        labels = []\n",
    "        for i in range(len(self.data)):\n",
    "            for key in self.data[i].split('||'):\n",
    "                if key.strip() != '':\n",
    "                    key = key.strip().replace('\\n','')\n",
    "                    texts.append(key) \n",
    "                    knowledges.append(self.knowledge[i].strip().replace('\\n',''))\n",
    "                    labels.append(self.label[i].strip().replace('\\n',''))\n",
    "        text_csv = pd.DataFrame({'text':texts, 'knowledge':knowledges, 'label':labels})\n",
    "        text_csv.to_csv(text_path, index = False, encoding = 'utf-8')\n",
    "        \n",
    "    def split_real_labeled_unlabeld_and_test(self):\n",
    "        self.get_texts(self.text_path)\n",
    "        data = pd.read_csv(self.text_path)\n",
    "        l = list(data['text'])\n",
    "        k = list(data['knowledge'])\n",
    "        lab = list(data['label'])\n",
    "        '''\n",
    "        res = []\n",
    "        for i in range(len(l)):\n",
    "            res.append((l[i], i))\n",
    "        pickle.dump(res, open('total_texts.txt', 'w'))\n",
    "        '''\n",
    "        unique_knowledge = data['knowledge'].unique()\n",
    "        self.labeled_indexes = []\n",
    "        #data = data[data['label'] != '聊天']\n",
    "        for i in range(len(unique_knowledge)):\n",
    "            data_of_know = data[data['knowledge'] == unique_knowledge[i]]\n",
    "            inds = data_of_know.index.tolist()\n",
    "            random.shuffle(inds)\n",
    "            if len(inds) < 10:\n",
    "                self.labeled_indexes = self.labeled_indexes + inds[:int(0.5*len(inds)) + 1]\n",
    "            else:\n",
    "                self.labeled_indexes = self.labeled_indexes + inds[:10]\n",
    "        self.labeled_indexes = data.index.tolist()\n",
    "        data = pd.concat([pd.read_csv(self.text_path), pd.read_csv('../data/undefined_text.csv')])\n",
    "        data = data.reset_index()\n",
    "        data = data.drop('index', axis = 1)\n",
    "        self.indexes = data.index.tolist()\n",
    "        #random.shuffle(left_indexes)\n",
    "        self.unlabeled_indexes = list(set(self.indexes) - set(self.labeled_indexes))\n",
    "        labeled, unlabeled = data.loc[self.labeled_indexes], data.loc[self.unlabeled_indexes]\n",
    "        print len(self.unlabeled_indexes),len(self.labeled_indexes),len(self.indexes)\n",
    "        return labeled, unlabeled\n",
    "    \n",
    "    def split_labeled_unlabeld_and_test(self):\n",
    "        self.get_texts(self.text_path)\n",
    "        data = pd.read_csv(self.text_path)\n",
    "        l = list(data['text'])\n",
    "        k = list(data['knowledge'])\n",
    "        lab = list(data['label'])\n",
    "        '''\n",
    "        res = []\n",
    "        for i in range(len(l)):\n",
    "            res.append((l[i], i))\n",
    "        pickle.dump(res, open('total_texts.txt', 'w'))\n",
    "        \n",
    "        import ipdb;\n",
    "        ipdb.set_trace()\n",
    "        '''\n",
    "        unique_knowledge = data['knowledge'].unique()\n",
    "        self.labeled_indexes = []\n",
    "        #data = data[data['label'] != '聊天']\n",
    "        for i in range(len(unique_knowledge)):\n",
    "            data_of_know = data[data['knowledge'] == unique_knowledge[i]]\n",
    "            inds = data_of_know.index.tolist()\n",
    "            random.shuffle(inds)\n",
    "            if len(inds) < 10:\n",
    "                self.labeled_indexes = self.labeled_indexes + inds[:int(0.5*len(inds)) + 1]\n",
    "            else:\n",
    "                self.labeled_indexes = self.labeled_indexes + inds[:10]\n",
    "            \n",
    "        self.indexes = data.index.tolist()\n",
    "        left_indexes = list(set(self.indexes) - set(self.labeled_indexes))\n",
    "        self.unlabeled_indexes, self.test_indexes = left_indexes[:int(0.8 * len(left_indexes))], left_indexes[int(0.8 * len(left_indexes)) :]\n",
    "        labeled, unlabeled, test = data.loc[self.labeled_indexes], data.loc[self.unlabeled_indexes], data.loc[self.test_indexes]\n",
    "        new_data = pd.concat([labeled, unlabeled, test], axis = 0)\n",
    "        new_data = new_data.reset_index()\n",
    "        new_data = new_data.drop('index', axis = 1)\n",
    "        labeled = new_data.loc[new_data.index.tolist()[len(new_data) - len(test):]]\n",
    "        unlabeled = new_data.loc[new_data.index.tolist()[:len(new_data) - len(test)]]\n",
    "        labeled.to_csv('../data/test_undefined.csv')\n",
    "        unlabeled.to_csv('../data/test_labeled.csv')\n",
    "        self.unlabeled_indexes = new_data.index.tolist()[:len(new_data) - len(test)]\n",
    "        return labeled, unlabeled\n",
    "    \n",
    "    def get_dict_of_questions(self):\n",
    "        self.dic = pickle.load(open('../data/dic_results_500.txt', 'r'))\n",
    "        for i in range(len(self.indexes)):\n",
    "            sim_result = self.dic[self.indexes[i]]\n",
    "            scores = [sim_result[k][1] for k in range(len(sim_result))]\n",
    "            for m in range(len(scores)):\n",
    "                for j in range(len(self.range) - 1):\n",
    "                    if self.range[j] < float(scores[m]) <= self.range[j + 1]:\n",
    "                        self.range_result[self.range[j + 1]] += 1\n",
    "        xl = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0]\n",
    "        col_value = []\n",
    "        for i in range(len(xl)):\n",
    "            col_value.append(self.range_result[xl[i]])\n",
    "        plt.bar(range(5,105,5), col_value)\n",
    "        plt.show()\n",
    "        return self.dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class active_learning():\n",
    "    \n",
    "    def __init__(self, labeled, unlabeled, test, dic, unlabeled_indexes, k, word_util):\n",
    "        self.labeled = labeled\n",
    "        self.unlabeled = unlabeled\n",
    "        self.test = test\n",
    "        self.labeled_texts = list(labeled['text'])\n",
    "        self.unlabeled_texts = list(unlabeled['text'])\n",
    "        #self.test_texts = list(test['text'])\n",
    "        self.dic = dic\n",
    "        #self.real = list(test['knowledge'])\n",
    "        self.unlabeled_indexes = unlabeled_indexes\n",
    "        self.k = k\n",
    "        self.sets = []\n",
    "        self.word_util = word_util\n",
    "        \n",
    "    def predict(self):\n",
    "        pred = []\n",
    "        print 'now labeled sets size is : ' + str(len(self.labeled_texts))\n",
    "        for i in self.test.index.tolist():\n",
    "            results = self.dic[i]\n",
    "            res_in = []\n",
    "            for res in results:\n",
    "                ques, score = res\n",
    "                if ques.encode('utf-8') in self.labeled_texts:\n",
    "                    knowledge = self.labeled[self.labeled['text'] == ques.encode('utf-8')]['knowledge']\n",
    "                    knowledge = knowledge.loc[knowledge.index.tolist()[0]]\n",
    "                    res_in.append(knowledge)\n",
    "            if len(res_in) == 0:\n",
    "                res_in.append('聊天')\n",
    "            sort_dic = sorted(Counter(res_in).items(), key = lambda item:item[1], reverse = True)\n",
    "            final_pred = sort_dic[0][0]\n",
    "            pred.append(final_pred)\n",
    "        real = [self.real[i].decode('utf-8') for i in range(len(self.real))]\n",
    "        preds = [pred[i].decode('utf-8') for i in range(len(pred))]\n",
    "        print('classification_report :\\n%s' % metrics.classification_report(real, preds)) \n",
    "        \n",
    "    def select_set(self):\n",
    "        ent_dict = {}\n",
    "        for i in self.unlabeled_indexes:\n",
    "            results = dic[i]\n",
    "            res_in = []\n",
    "            for res in results:\n",
    "                ques, score = res\n",
    "                if ques.encode('utf-8') in self.labeled_texts:\n",
    "                    knowledge = self.labeled[self.labeled['text'] == ques.encode('utf-8')]['knowledge']\n",
    "                    knowledge = knowledge.loc[knowledge.index.tolist()[0]]\n",
    "                    res_in.append((knowledge, float(score)))\n",
    "            if len(res_in) == 0:\n",
    "                res_in.append(('聊天',0.0000001))\n",
    "            counter = {}\n",
    "            total_score = 0.0000001\n",
    "            for k in range(len(res_in)):\n",
    "                if res_in[k][0] not in counter:\n",
    "                    counter[res_in[k][0]] = 0.0000001\n",
    "                counter[res_in[k][0]] += res_in[k][1]\n",
    "                total_score += res_in[k][1]\n",
    "            ent = 0.0000001\n",
    "            scores,ents, ent_dic = [], [], {}\n",
    "            for key in counter:\n",
    "                scores.append(counter[key] / total_score)\n",
    "                p = counter[key] / total_score\n",
    "                logp = np.log2(p)\n",
    "                ent -= p * logp\n",
    "            ents.append(ent)            \n",
    "            ent_dict[i] = ent\n",
    "        sort_dic = sorted(ent_dict.items(), key = lambda item:item[1], reverse = True)[:self.k]\n",
    "        cur_set_index = [sort_dic[k][0] for k in range(len(sort_dic))]\n",
    "        return cur_set_index\n",
    "            \n",
    "    def merge(self, cur_set):\n",
    "        self.sets = [] \n",
    "        new_labeled = pd.concat((self.labeled, self.unlabeled.loc[cur_set]), axis = 0)\n",
    "        self.unlabeled.loc[cur_set,'text'].to_csv('../data/new_labeled' + str(len(new_labeled)) + '.csv')        \n",
    "        new_unlabeled = self.unlabeled.drop(cur_set, axis = 0)\n",
    "        self.labeled = new_labeled\n",
    "        self.unlabeled = new_unlabeled\n",
    "        self.labeled_texts = list(self.labeled['text'])\n",
    "        self.unlabeled_texts = list(self.unlabeled['text'])\n",
    "        self.unlabeled_indexes = new_unlabeled.index.tolist()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVK3/wPMOPmlP9sAlwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of cur unlabeled index is : 9650\n",
      "> \u001b[0;32m<ipython-input-6-b07b7a7c74d1>\u001b[0m(62)\u001b[0;36mselect_set\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     61 \u001b[0;31m            \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 62 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     63 \u001b[0;31m                \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    data_util = data('../data/data.txt', '../data/text.csv', '../data/knowledge.txt', '../data/label.txt')\n",
    "    word_util = word()\n",
    "    word_util.get_word2vec()\n",
    "    labeled, unlabeled = data_util.split_labeled_unlabeld_and_test()\n",
    "    #labeled, unlabeled = data_util.split_real_labeled_unlabeld_and_test()\n",
    "    test = None\n",
    "    labeled_texts = labeled['text']\n",
    "    #test_texts = test['text']\n",
    "    dic = data_util.get_dict_of_questions()\n",
    "    labeled_indexes, unlabeled_indexes = data_util.labeled_indexes, data_util.unlabeled_indexes\n",
    "    al = active_learning(labeled, unlabeled, test, dic, unlabeled_indexes, 500, word_util)\n",
    "    while len(al.unlabeled_indexes) > 500:\n",
    "        print 'length of cur unlabeled index is : ' + str(len(al.unlabeled_indexes))\n",
    "        #al.predict()\n",
    "        cur_set = al.select_set()\n",
    "        al.merge(cur_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfenv.v1.2]",
   "language": "python",
   "name": "conda-env-tfenv.v1.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
