{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/lil.py:19: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _csparsetools\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:165: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._shortest_path import shortest_path, floyd_warshall, dijkstra,\\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._tools import csgraph_to_dense, csgraph_from_dense,\\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:167: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._traversal import breadth_first_order, depth_first_order, \\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:169: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._min_spanning_tree import minimum_spanning_tree\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:170: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._reordering import reverse_cuthill_mckee, maximum_bipartite_matching, \\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/special/__init__.py:640: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ufuncs import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/linalg/basic.py:17: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._solve_toeplitz import levinson\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/linalg/__init__.py:207: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._decomp_update import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/special/_ellip_harm.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/interpolate/_bsplines.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _bspl\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/__init__.py:95: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .ckdtree import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/__init__.py:96: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .qhull import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/_spherical_voronoi.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _voronoi\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/distance.py:122: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _hausdorff\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/optimize/_trlib/__init__.py:1: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._trlib import TRLIBQuadraticSubproblem\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/optimize/_numdiff.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._group_columns import group_dense, group_sparse\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/stats/_continuous_distns.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _stats\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/__init__.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/__init__.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/_hl/group.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .. import h5g, h5i, h5o, h5r, h5t, h5l, h5p\n",
      "Using TensorFlow backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.263 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/utils/__init__.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .murmurhash import murmurhash3_32\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/utils/extmath.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._logistic_sigmoid import _log_logistic_sigmoid\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/utils/extmath.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .sparsefuncs_fast import csr_row_norms\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/metrics/cluster/supervised.py:23: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .expected_mutual_info_fast import expected_mutual_information\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 1006 user-define jieba dict success!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from intent_classify import IntentClassify\n",
    "from gensim import corpora, models, similarities,matutils\n",
    "from fasttext_util import FasttextClassifier\n",
    "import time\n",
    "from collections import Counter\n",
    "import re\n",
    "from url import Searcher\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word():    \n",
    "    def __init__(self):\n",
    "        self.word_to_vec = {}\n",
    "        self.dictionary = corpora.dictionary.Dictionary()\n",
    "        self.dic = self.dictionary.load('../util/new/laiye/corpus.dict')\n",
    "        self.tfidf = models.TfidfModel.load('../util/new/laiye/corpus.tfidf_model')  \n",
    "        \n",
    "    def remove_punctuation(self, line):\n",
    "        rule = re.compile(ur\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")\n",
    "        line = rule.sub('',line)\n",
    "        return line        \n",
    "\n",
    "    def stopwordslist(self, filepath):  \n",
    "        stopwords = [line.strip() for line in open(filepath, 'r').readlines()]  \n",
    "        return stopwords  \n",
    "\n",
    "\n",
    "    # 对句子进行分词  \n",
    "    def jieba_cut(self, sentence):  \n",
    "        sentence_seged = jieba.cut(sentence.strip())  \n",
    "        stopwords = stopwordslist('../util/new/laiye/stopwords.txt')  # 这里加载停用词的路径  \n",
    "        outstr = ''  \n",
    "        for word in sentence_seged:  \n",
    "            if word.encode('utf-8') not in stopwords:  \n",
    "                if word.encode('utf-8') != '\\t' and word != '\\t':  \n",
    "                    outstr += word\n",
    "                    outstr += \" \"  \n",
    "        return outstr.strip().split(' ')\n",
    "\n",
    "    def get_word2vec(self):\n",
    "        with open('../util/new/laiye/w2v_sgns_win1_d80.kv') as f:\n",
    "            data = [x.split(' ') for x in f.readlines()[1:]]\n",
    "            words = [d[0] for d in data]\n",
    "            vecs = np.array([d[1: -1] for d in data], dtype= 'float64')\n",
    "            for i in range(len(words)):\n",
    "                word = words[i]\n",
    "                self.word_to_vec[word] = vecs[i]\n",
    "\n",
    "    def get_tf_idf_of_query(self, query, dic, tfidf):      \n",
    "        vec_bow = dic.doc2bow(query)\n",
    "        vec_tfidf = tfidf[vec_bow]\n",
    "        tp = [0.0] * len(query)\n",
    "        ids = [tid[0] for tid in vec_tfidf]\n",
    "        flags = [0] * len(query)\n",
    "        count = 0.00001\n",
    "        for j in range(len(query)):\n",
    "            for i in range(len(ids)):\n",
    "                if self.dic[ids[i]] == query[j]:\n",
    "                    tp[j] = vec_tfidf[i][1]\n",
    "                    flags[j] = 1\n",
    "                    count += 1\n",
    "                    break\n",
    "        sums = sum(tp)\n",
    "        #print ','.join(query)\n",
    "        for i in range(len(flags)):\n",
    "            if flags[i] == 1:\n",
    "                continue\n",
    "            tp[i] = 1.0 * sums / count\n",
    "                    \n",
    "        # apply l1-norm to tfidf value\n",
    "        #tfidf = matutils.unitvec(vec_tfidf, norm = 'l1')   \n",
    "        return tp\n",
    "\n",
    "    def get_vectors_of_data_cut(self, data):\n",
    "        vecs = []\n",
    "        for i in range(len(data)):\n",
    "            vec = []\n",
    "            d_line = data[i].split(' ')\n",
    "            tfidfs = self.get_tf_idf_of_query(d_line, self.dic, self.tfidf)\n",
    "            s = sum(tfidfs)\n",
    "            for j in range(len(d_line)):\n",
    "                if d_line[j].encode('utf-8') in self.word_to_vec:\n",
    "                    if s == 0:\n",
    "                        vec.append([0.0] * len(self.word_to_vec['家']))\n",
    "                    else:\n",
    "                        vec.append((tfidfs[j] / s) * self.word_to_vec[d_line[j].encode('utf-8')])\n",
    "            if len(vec) == 0:\n",
    "                vec.append([0.0] * len(self.word_to_vec['家']))\n",
    "            vecs.append(np.sum(np.array(vec), axis = 0))\n",
    "        return np.array(vecs)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data():\n",
    "    \n",
    "    def __init__(self, data_path, text_path, knowledge_path, label_path):\n",
    "        self.data = open(data_path, 'r').readlines()\n",
    "        self.knowledge = open(knowledge_path, 'r').readlines()\n",
    "        self.text_path = text_path\n",
    "        self.label = open(label_path, 'r').readlines()\n",
    "        self.indexes = []\n",
    "        self.labeled_indexes = []\n",
    "        self.unlabeled_indexes = []\n",
    "        self.test_indexes = []\n",
    "        self.questions_match_dict = []\n",
    "        self.t_map = {}\n",
    "        self.range = [0.000001,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0]\n",
    "        self.range_result = {0.05:0, 0.1:0,0.15:0, 0.2:0,0.25:0, 0.3:0,0.35:0, 0.4:0,0.45:0,0.5:0, 0.55:0,0.6:0,0.65:0,0.7:0, 0.75:0,0.8:0,0.85:0,0.9:0,0.95:0, 1.0:0}\n",
    "        self.dic = {}\n",
    "        \n",
    "    def get_texts(self, text_path):\n",
    "        texts = []\n",
    "        knowledges = []\n",
    "        labels = []\n",
    "        for i in range(len(self.data)):\n",
    "            for key in self.data[i].split('||'):\n",
    "                if key.strip() != '':\n",
    "                    key = key.strip().replace('\\n','')\n",
    "                    texts.append(key) \n",
    "                    knowledges.append(self.knowledge[i].strip().replace('\\n',''))\n",
    "                    labels.append(self.label[i].strip().replace('\\n',''))\n",
    "        text_csv = pd.DataFrame({'text':texts, 'knowledge':knowledges, 'label':labels})\n",
    "        text_csv.to_csv(text_path, index = False, encoding = 'utf-8')\n",
    "        \n",
    "    def split_real_labeled_unlabeld_and_test(self):\n",
    "        self.get_texts(self.text_path)\n",
    "        data = pd.read_csv(self.text_path)\n",
    "        l = list(data['text'])\n",
    "        k = list(data['knowledge'])\n",
    "        lab = list(data['label'])\n",
    "        '''\n",
    "        res = []\n",
    "        for i in range(len(l)):\n",
    "            res.append((l[i], i))\n",
    "        pickle.dump(res, open('total_texts.txt', 'w'))\n",
    "        '''\n",
    "        undefined = pd.read_csv('../data/undefined_text.csv')\n",
    "        labeled, unlabeled = data, undefined\n",
    "        return labeled, unlabeled\n",
    "    \n",
    "    def split_labeled_unlabeld_and_test(self):\n",
    "        self.get_texts(self.text_path)\n",
    "        data = pd.read_csv(self.text_path)\n",
    "        l = list(data['text'])\n",
    "        k = list(data['knowledge'])\n",
    "        lab = list(data['label'])\n",
    "        '''\n",
    "        res = []\n",
    "        for i in range(len(l)):\n",
    "            res.append((l[i], i))\n",
    "        pickle.dump(res, open('total_texts.txt', 'w'))\n",
    "        \n",
    "        import ipdb;\n",
    "        ipdb.set_trace()\n",
    "        '''\n",
    "        unique_knowledge = data['knowledge'].unique()\n",
    "        self.labeled_indexes = []\n",
    "        #data = data[data['label'] != '聊天']\n",
    "        for i in range(len(unique_knowledge)):\n",
    "            data_of_know = data[data['knowledge'] == unique_knowledge[i]]\n",
    "            inds = data_of_know.index.tolist()\n",
    "            random.shuffle(inds)\n",
    "            if len(inds) < 10:\n",
    "                self.labeled_indexes = self.labeled_indexes + inds[:int(0.5*len(inds)) + 1]\n",
    "            else:\n",
    "                self.labeled_indexes = self.labeled_indexes + inds[:10]\n",
    "            \n",
    "        self.indexes = data.index.tolist()\n",
    "        left_indexes = list(set(self.indexes) - set(self.labeled_indexes))\n",
    "        random.shuffle(left_indexes)\n",
    "        self.unlabeled_indexes, self.test_indexes = left_indexes[:int(0.8 * len(left_indexes))], left_indexes[int(0.8 * len(left_indexes)) :]\n",
    "        labeled, unlabeled, test = data.loc[self.labeled_indexes], data.loc[self.unlabeled_indexes], data.loc[self.test_indexes]\n",
    "        print len(self.unlabeled_indexes),len(self.test_indexes),len(self.labeled_indexes),len(self.indexes)\n",
    "        test.to_csv('../data/test_undefined.csv')\n",
    "        return labeled, unlabeled, test\n",
    "    \n",
    "    def trans_i_to_str_dic(self, dic):\n",
    "        i_to_str_map = pickle.load(open('total_texts.txt'))\n",
    "        for i in range(len(i_to_str_map)):\n",
    "            self.t_map[i_to_str_map[i][1]] = i_to_str_map[i][0]\n",
    "        new_dic = {}\n",
    "        for key in dic:\n",
    "            new_dic[self.t_map[key]] = dic[key]\n",
    "        return new_dic\n",
    "    \n",
    "    def get_dict_of_questions(self):\n",
    "        self.dic = pickle.load(open('../data/xiaolai100000+.txt', 'r'))#pickle.load(open('../data/total_dic_results.txt', 'r'))\n",
    "        for key in self.dic:\n",
    "            sim_result = self.dic[key]\n",
    "            scores = [sim_result[k][2] for k in range(len(sim_result))]\n",
    "            for m in range(len(scores)):\n",
    "                for j in range(len(self.range) - 1):\n",
    "                    if self.range[j] < float(scores[m]) <= self.range[j + 1]:\n",
    "                        self.range_result[self.range[j + 1]] += 1\n",
    "        xl = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0]\n",
    "        col_value = []\n",
    "        for i in range(len(xl)):\n",
    "            col_value.append(self.range_result[xl[i]])\n",
    "        plt.bar(range(5,105,5), col_value)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        def update_unlabeled_data(self, new_undefined_path):\n",
    "            origin_texts = [key for key in self.dic]\n",
    "            cur_undefined = pd.read_csv(new_undefined_path)\n",
    "            cur_texts = list(new_undefined['text'])\n",
    "            new_texts = list(set(cur_texts) - set(origin_texts))\n",
    "            return new_texts\n",
    "        \n",
    "        return self.dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class active_learning():\n",
    "    \n",
    "    def __init__(self, labeled, unlabeled, test, dic,  k, word_util, map_):\n",
    "        self.labeled = labeled\n",
    "        self.unlabeled = unlabeled\n",
    "        self.test = test\n",
    "        self.labeled_texts = list(labeled['text'])\n",
    "        self.unlabeled_texts = list(unlabeled['text'])\n",
    "        #self.test_texts = list(test['text'])\n",
    "        self.dic = dic\n",
    "        #self.real = list(test['knowledge'])\n",
    "        self.unlabeled_indexes = unlabeled.index.tolist()\n",
    "        self.k = k\n",
    "        self.sets = []\n",
    "        self.map = map_\n",
    "        self.word_util = word_util\n",
    "        \n",
    "    def predict(self):\n",
    "        pred = []\n",
    "        print 'now labeled sets size is : ' + str(len(self.labeled_texts))\n",
    "        for i in self.test.index.tolist():\n",
    "            results = self.dic[i]\n",
    "            res_in = []\n",
    "            for res in results:\n",
    "                ques, title, score = res\n",
    "                if ques.encode('utf-8') in self.labeled_texts:\n",
    "                    knowledge = self.labeled[self.labeled['text'] == ques.encode('utf-8')]['knowledge']\n",
    "                    knowledge = knowledge.loc[knowledge.index.tolist()[0]]\n",
    "                    res_in.append(knowledge)\n",
    "            if len(res_in) == 0:\n",
    "                res_in.append('聊天')\n",
    "            sort_dic = sorted(Counter(res_in).items(), key = lambda item:item[1], reverse = True)\n",
    "            final_pred = sort_dic[0][0]\n",
    "            pred.append(final_pred)\n",
    "        real = [self.real[i].decode('utf-8') for i in range(len(self.real))]\n",
    "        preds = [pred[i].decode('utf-8') for i in range(len(pred))]\n",
    "        print('classification_report :\\n%s' % metrics.classification_report(real, preds)) \n",
    "        \n",
    "    def select_set(self):\n",
    "        ent_dict = {}\n",
    "        count_dict = []\n",
    "        for i in self.unlabeled_indexes[:100001]:\n",
    "            if self.map[i] not in dic:\n",
    "                print i, self.map[i]\n",
    "                break\n",
    "            results = dic[self.map[i]]\n",
    "            res_in = []\n",
    "            if i % 100 == 0:\n",
    "                print i\n",
    "            for res in results:\n",
    "                ques, title, score = res\n",
    "                if ques.encode('utf-8') in self.labeled_texts and 0.45 <= float(score):\n",
    "                    knowledge = self.labeled[self.labeled['text'] == ques.encode('utf-8')]['knowledge']\n",
    "                    knowledge = knowledge.loc[knowledge.index.tolist()[0]]\n",
    "                    res_in.append((knowledge, float(score)))\n",
    "            if len(res_in) == 0:\n",
    "                res_in.append(('聊天',0.0000001))\n",
    "            counter = {}\n",
    "            total_score = 0.0000001\n",
    "            for k in range(len(res_in)):\n",
    "                if res_in[k][0] not in counter:\n",
    "                    counter[res_in[k][0]] = 0.0000001\n",
    "                counter[res_in[k][0]] += res_in[k][1]\n",
    "                total_score += res_in[k][1]\n",
    "            ent = 0.0000001\n",
    "            scores,ents, ent_dic = [], [], {}\n",
    "            for key in counter:\n",
    "                scores.append(counter[key] / total_score)\n",
    "                p = counter[key] / total_score\n",
    "                logp = np.log2(p)\n",
    "                ent -= p * logp\n",
    "            ents.append(ent)     \n",
    "            ent_dict[i] = ent\n",
    "            count_dict.append(counter)\n",
    "        sort_dic = sorted(ent_dict.items(), key = lambda item:item[1], reverse = True)[:self.k]\n",
    "        cur_set_index = [sort_dic[k][0] for k in range(len(sort_dic))]\n",
    "        cur_set_entropy = [sort_dic[k][1] for k in range(len(sort_dic))]\n",
    "        cur_count = []\n",
    "        for i in range(len(count_dict)):\n",
    "            if i in cur_set_index:\n",
    "                cur_count.append(count_dict[i])\n",
    "        \n",
    "        return cur_set_index, cur_set_entropy, cur_count\n",
    "            \n",
    "    def merge(self, cur_set, cur_set_entropy, cur_count):\n",
    "        self.sets = [] \n",
    "        new_labeled = pd.concat((self.labeled, self.unlabeled.loc[cur_set]), axis = 0)\n",
    "        dt = pd.DataFrame({'text':[], 'entropy':[], 'index':[], 'count' :[]})\n",
    "        print 'merge into file : '+ '../data/total_results/entropy_sum' + str(len(new_labeled)) + '.csv'\n",
    "        dt['text'] = list(self.unlabeled.loc[cur_set,'text'])\n",
    "        dt['count'] = cur_count\n",
    "        dt['entropy'] = cur_set_entropy\n",
    "        dt['index'] = self.unlabeled.loc[cur_set].index.tolist()\n",
    "        dt.to_csv('../data/total_results/entropy_sum' + str(len(new_labeled)) + '.csv', index = False, encoding = 'utf-8')   \n",
    "        new_unlabeled = self.unlabeled.drop(cur_set, axis = 0)\n",
    "        self.labeled = new_labeled\n",
    "        self.unlabeled = new_unlabeled\n",
    "        self.labeled_texts = list(self.labeled['text'])\n",
    "        self.unlabeled_texts = list(self.unlabeled['text'])\n",
    "        self.unlabeled_indexes = new_unlabeled.index.tolist()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if __name__==\"__main__\":\n",
    "data_util = data('../data/data.txt', '../data/text.csv', '../data/knowledge.txt', '../data/label.txt')\n",
    "word_util = word()\n",
    "word_util.get_word2vec()\n",
    "#labeled, unlabeled, test = data_util.split_labeled_unlabeld_and_test()\n",
    "labeled, unlabeled = data_util.split_real_labeled_unlabeld_and_test()\n",
    "test = None\n",
    "labeled_texts = labeled['text']\n",
    "#test_texts = test['text']\n",
    "#dic = data_util.get_dict_of_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nR3wwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_dic = data_util.get_dict_of_questions()\n",
    "dic = data_util.trans_i_to_str_dic(t_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of cur unlabeled index is : 104983\n",
      "0\n",
      "100\n",
      "merge into file : ../data/total_results/entropy_sum11217.csv\n"
     ]
    }
   ],
   "source": [
    "labeled_indexes, unlabeled_indexes = data_util.labeled_indexes, data_util.unlabeled_indexes\n",
    "al = active_learning(labeled, unlabeled, test, dic, 500, word_util, data_util.t_map)\n",
    "while len(al.unlabeled_indexes) > 500:\n",
    "    print 'length of cur unlabeled index is : ' + str(len(al.unlabeled_indexes))\n",
    "    #al.predict()\n",
    "    cur_set, cur_set_entropy, cur_count = al.select_set()\n",
    "    al.merge(cur_set, cur_set_entropy, cur_count)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfenv.v1.2]",
   "language": "python",
   "name": "conda-env-tfenv.v1.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
