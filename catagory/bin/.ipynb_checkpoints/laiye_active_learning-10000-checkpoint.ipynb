{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/lil.py:19: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _csparsetools\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:165: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._shortest_path import shortest_path, floyd_warshall, dijkstra,\\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._tools import csgraph_to_dense, csgraph_from_dense,\\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:167: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._traversal import breadth_first_order, depth_first_order, \\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:169: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._min_spanning_tree import minimum_spanning_tree\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:170: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._reordering import reverse_cuthill_mckee, maximum_bipartite_matching, \\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/special/__init__.py:640: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ufuncs import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/linalg/basic.py:17: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._solve_toeplitz import levinson\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/linalg/__init__.py:207: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._decomp_update import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/special/_ellip_harm.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/interpolate/_bsplines.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _bspl\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/__init__.py:95: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .ckdtree import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/__init__.py:96: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .qhull import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/_spherical_voronoi.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _voronoi\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/distance.py:122: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _hausdorff\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/optimize/_trlib/__init__.py:1: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._trlib import TRLIBQuadraticSubproblem\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/optimize/_numdiff.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._group_columns import group_dense, group_sparse\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/stats/_continuous_distns.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _stats\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/__init__.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/__init__.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/_hl/group.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .. import h5g, h5i, h5o, h5r, h5t, h5l, h5p\n",
      "Using TensorFlow backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.256 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/utils/__init__.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .murmurhash import murmurhash3_32\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/utils/extmath.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._logistic_sigmoid import _log_logistic_sigmoid\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/utils/extmath.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .sparsefuncs_fast import csr_row_norms\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/metrics/cluster/supervised.py:23: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .expected_mutual_info_fast import expected_mutual_information\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 1006 user-define jieba dict success!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from intent_classify import IntentClassify\n",
    "from gensim import corpora, models, similarities,matutils\n",
    "from fasttext_util import FasttextClassifier\n",
    "import time\n",
    "from collections import Counter\n",
    "import re\n",
    "from url import Searcher\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word():    \n",
    "    def __init__(self):\n",
    "        self.word_to_vec = {}\n",
    "        self.dictionary = corpora.dictionary.Dictionary()\n",
    "        self.dic = self.dictionary.load('../util/new/laiye/corpus.dict')\n",
    "        self.tfidf = models.TfidfModel.load('../util/new/laiye/corpus.tfidf_model')  \n",
    "        \n",
    "    def remove_punctuation(self, line):\n",
    "        rule = re.compile(ur\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")\n",
    "        line = rule.sub('',line)\n",
    "        return line        \n",
    "\n",
    "    def stopwordslist(self, filepath):  \n",
    "        stopwords = [line.strip() for line in open(filepath, 'r').readlines()]  \n",
    "        return stopwords  \n",
    "\n",
    "\n",
    "    # 对句子进行分词  \n",
    "    def jieba_cut(self, sentence):  \n",
    "        sentence_seged = jieba.cut(sentence.strip())  \n",
    "        stopwords = stopwordslist('../util/new/laiye/stopwords.txt')  # 这里加载停用词的路径  \n",
    "        outstr = ''  \n",
    "        for word in sentence_seged:  \n",
    "            if word.encode('utf-8') not in stopwords:  \n",
    "                if word.encode('utf-8') != '\\t' and word != '\\t':  \n",
    "                    outstr += word\n",
    "                    outstr += \" \"  \n",
    "        return outstr.strip().split(' ')\n",
    "\n",
    "    def get_word2vec(self):\n",
    "        with open('../util/new/laiye/w2v_sgns_win1_d80.kv') as f:\n",
    "            data = [x.split(' ') for x in f.readlines()[1:]]\n",
    "            words = [d[0] for d in data]\n",
    "            vecs = np.array([d[1: -1] for d in data], dtype= 'float64')\n",
    "            for i in range(len(words)):\n",
    "                word = words[i]\n",
    "                self.word_to_vec[word] = vecs[i]\n",
    "\n",
    "    def get_tf_idf_of_query(self, query, dic, tfidf):      \n",
    "        vec_bow = dic.doc2bow(query)\n",
    "        vec_tfidf = tfidf[vec_bow]\n",
    "        tp = [0.0] * len(query)\n",
    "        ids = [tid[0] for tid in vec_tfidf]\n",
    "        flags = [0] * len(query)\n",
    "        count = 0.00001\n",
    "        for j in range(len(query)):\n",
    "            for i in range(len(ids)):\n",
    "                if self.dic[ids[i]] == query[j]:\n",
    "                    tp[j] = vec_tfidf[i][1]\n",
    "                    flags[j] = 1\n",
    "                    count += 1\n",
    "                    break\n",
    "        sums = sum(tp)\n",
    "        #print ','.join(query)\n",
    "        for i in range(len(flags)):\n",
    "            if flags[i] == 1:\n",
    "                continue\n",
    "            tp[i] = 1.0 * sums / count\n",
    "                    \n",
    "        # apply l1-norm to tfidf value\n",
    "        #tfidf = matutils.unitvec(vec_tfidf, norm = 'l1')   \n",
    "        return tp\n",
    "\n",
    "    def get_vectors_of_data_cut(self, data):\n",
    "        vecs = []\n",
    "        for i in range(len(data)):\n",
    "            vec = []\n",
    "            d_line = data[i].split(' ')\n",
    "            tfidfs = self.get_tf_idf_of_query(d_line, self.dic, self.tfidf)\n",
    "            s = sum(tfidfs)\n",
    "            for j in range(len(d_line)):\n",
    "                if d_line[j].encode('utf-8') in self.word_to_vec:\n",
    "                    if s == 0:\n",
    "                        vec.append([0.0] * len(self.word_to_vec['家']))\n",
    "                    else:\n",
    "                        vec.append((tfidfs[j] / s) * self.word_to_vec[d_line[j].encode('utf-8')])\n",
    "            if len(vec) == 0:\n",
    "                vec.append([0.0] * len(self.word_to_vec['家']))\n",
    "            vecs.append(np.sum(np.array(vec), axis = 0))\n",
    "        return np.array(vecs)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data():\n",
    "    \n",
    "    def __init__(self, data_path, text_path, knowledge_path, label_path):\n",
    "        self.data = open(data_path, 'r').readlines()\n",
    "        self.knowledge = open(knowledge_path, 'r').readlines()\n",
    "        self.text_path = text_path\n",
    "        self.label = open(label_path, 'r').readlines()\n",
    "        self.indexes = []\n",
    "        self.labeled_indexes = []\n",
    "        self.unlabeled_indexes = []\n",
    "        self.test_indexes = []\n",
    "        self.questions_match_dict = []\n",
    "        self.range = [0.000001,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0]\n",
    "        self.range_result = {0.05:0, 0.1:0,0.15:0, 0.2:0,0.25:0, 0.3:0,0.35:0, 0.4:0,0.45:0,0.5:0, 0.55:0,0.6:0,0.65:0,0.7:0, 0.75:0,0.8:0,0.85:0,0.9:0,0.95:0, 1.0:0}\n",
    "        self.dic = {}\n",
    "        \n",
    "    def get_texts(self, text_path):\n",
    "        texts = []\n",
    "        knowledges = []\n",
    "        labels = []\n",
    "        for i in range(len(self.data)):\n",
    "            for key in self.data[i].split('||'):\n",
    "                if key.strip() != '':\n",
    "                    key = key.strip().replace('\\n','')\n",
    "                    texts.append(key) \n",
    "                    knowledges.append(self.knowledge[i].strip().replace('\\n',''))\n",
    "                    labels.append(self.label[i].strip().replace('\\n',''))\n",
    "        text_csv = pd.DataFrame({'text':texts, 'knowledge':knowledges, 'label':labels})\n",
    "        text_csv.to_csv(text_path, index = False, encoding = 'utf-8')\n",
    "        \n",
    "    def split_real_labeled_unlabeld_and_test(self):\n",
    "        self.get_texts(self.text_path)\n",
    "        data = pd.read_csv(self.text_path)\n",
    "        l = list(data['text'])\n",
    "        k = list(data['knowledge'])\n",
    "        lab = list(data['label'])\n",
    "        '''\n",
    "        res = []\n",
    "        for i in range(len(l)):\n",
    "            res.append((l[i], i))\n",
    "        pickle.dump(res, open('total_texts.txt', 'w'))\n",
    "        '''\n",
    "        unique_knowledge = data['knowledge'].unique()\n",
    "        self.labeled_indexes = []\n",
    "        self.labeled_indexes = data.index.tolist()\n",
    "        data = pd.concat([pd.read_csv(self.text_path), pd.read_csv('../data/undefined_text.csv')])\n",
    "        data = data.reset_index()\n",
    "        data = data.drop('index', axis = 1)\n",
    "        self.indexes = data.index.tolist()\n",
    "        #random.shuffle(left_indexes)\n",
    "        self.unlabeled_indexes = list(set(self.indexes) - set(self.labeled_indexes))\n",
    "        labeled, unlabeled = data.loc[self.labeled_indexes], data.loc[self.unlabeled_indexes]\n",
    "        print len(self.unlabeled_indexes),len(self.labeled_indexes),len(self.indexes)\n",
    "        return labeled, unlabeled\n",
    "    \n",
    "    def split_labeled_unlabeld_and_test(self):\n",
    "        self.get_texts(self.text_path)\n",
    "        data = pd.read_csv(self.text_path)\n",
    "        l = list(data['text'])\n",
    "        k = list(data['knowledge'])\n",
    "        lab = list(data['label'])\n",
    "        '''\n",
    "        res = []\n",
    "        for i in range(len(l)):\n",
    "            res.append((l[i], i))\n",
    "        pickle.dump(res, open('total_texts.txt', 'w'))\n",
    "        \n",
    "        import ipdb;\n",
    "        ipdb.set_trace()\n",
    "        '''\n",
    "        unique_knowledge = data['knowledge'].unique()\n",
    "        self.labeled_indexes = []\n",
    "        #data = data[data['label'] != '聊天']\n",
    "        for i in range(len(unique_knowledge)):\n",
    "            data_of_know = data[data['knowledge'] == unique_knowledge[i]]\n",
    "            inds = data_of_know.index.tolist()\n",
    "            random.shuffle(inds)\n",
    "            if len(inds) < 10:\n",
    "                self.labeled_indexes = self.labeled_indexes + inds[:int(0.5*len(inds)) + 1]\n",
    "            else:\n",
    "                self.labeled_indexes = self.labeled_indexes + inds[:10]\n",
    "            \n",
    "        self.indexes = data.index.tolist()\n",
    "        left_indexes = list(set(self.indexes) - set(self.labeled_indexes))\n",
    "        random.shuffle(left_indexes)\n",
    "        self.unlabeled_indexes, self.test_indexes = left_indexes[:int(0.8 * len(left_indexes))], left_indexes[int(0.8 * len(left_indexes)) :]\n",
    "        labeled, unlabeled, test = data.loc[self.labeled_indexes], data.loc[self.unlabeled_indexes], data.loc[self.test_indexes]\n",
    "        print len(self.unlabeled_indexes),len(self.test_indexes),len(self.labeled_indexes),len(self.indexes)\n",
    "        test.to_csv('../data/test_undefined.csv')\n",
    "        return labeled, unlabeled, test\n",
    "    \n",
    "    def get_dict_of_questions(self):\n",
    "        self.dic = pickle.load(open('../data/total_dic_results.txt', 'r'))\n",
    "        for i in range(len(self.indexes)):\n",
    "            sim_result = self.dic[self.indexes[i]]\n",
    "            scores = [sim_result[k][1] for k in range(len(sim_result))]\n",
    "            for m in range(len(scores)):\n",
    "                for j in range(len(self.range) - 1):\n",
    "                    if self.range[j] < float(scores[m]) <= self.range[j + 1]:\n",
    "                        self.range_result[self.range[j + 1]] += 1\n",
    "        xl = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0]\n",
    "        col_value = []\n",
    "        for i in range(len(xl)):\n",
    "            col_value.append(self.range_result[xl[i]])\n",
    "        plt.bar(range(5,105,5), col_value)\n",
    "        plt.show()\n",
    "        return self.dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class active_learning():\n",
    "    \n",
    "    def __init__(self, labeled, unlabeled, test, dic, unlabeled_indexes, k, word_util):\n",
    "        self.labeled = labeled\n",
    "        self.unlabeled = unlabeled\n",
    "        self.test = test\n",
    "        self.labeled_texts = list(labeled['text'])\n",
    "        self.unlabeled_texts = list(unlabeled['text'])\n",
    "        #self.test_texts = list(test['text'])\n",
    "        self.dic = dic\n",
    "        #self.real = list(test['knowledge'])\n",
    "        self.unlabeled_indexes = unlabeled_indexes\n",
    "        self.k = k\n",
    "        self.sets = []\n",
    "        self.word_util = word_util\n",
    "        \n",
    "    def predict(self):\n",
    "        pred = []\n",
    "        print 'now labeled sets size is : ' + str(len(self.labeled_texts))\n",
    "        for i in self.test.index.tolist():\n",
    "            results = self.dic[i]\n",
    "            res_in = []\n",
    "            for res in results:\n",
    "                ques, score = res\n",
    "                if ques.encode('utf-8') in self.labeled_texts:\n",
    "                    knowledge = self.labeled[self.labeled['text'] == ques.encode('utf-8')]['knowledge']\n",
    "                    knowledge = knowledge.loc[knowledge.index.tolist()[0]]\n",
    "                    res_in.append(knowledge)\n",
    "            if len(res_in) == 0:\n",
    "                res_in.append('聊天')\n",
    "            sort_dic = sorted(Counter(res_in).items(), key = lambda item:item[1], reverse = True)\n",
    "            final_pred = sort_dic[0][0]\n",
    "            pred.append(final_pred)\n",
    "        real = [self.real[i].decode('utf-8') for i in range(len(self.real))]\n",
    "        preds = [pred[i].decode('utf-8') for i in range(len(pred))]\n",
    "        print('classification_report :\\n%s' % metrics.classification_report(real, preds)) \n",
    "        \n",
    "    def select_set(self):\n",
    "        ent_dict = {}\n",
    "        c = 0\n",
    "        for i in self.unlabeled_indexes:\n",
    "            c += 1\n",
    "            if c == 10000:\n",
    "                break\n",
    "            results = dic[i]\n",
    "            res_in = []\n",
    "            if i % 100 == 0:\n",
    "                print i\n",
    "            for res in results:\n",
    "                ques, score = res\n",
    "                if ques.encode('utf-8') in self.labeled_texts:\n",
    "                    knowledge = self.labeled[self.labeled['text'] == ques.encode('utf-8')]['knowledge']\n",
    "                    knowledge = knowledge.loc[knowledge.index.tolist()[0]]\n",
    "                    res_in.append((knowledge, float(score)))\n",
    "            if len(res_in) == 0:\n",
    "                res_in.append(('聊天',0.0000001))\n",
    "            counter = {}\n",
    "            total_score = 0.0000001\n",
    "            for k in range(len(res_in)):\n",
    "                if res_in[k][0] not in counter:\n",
    "                    counter[res_in[k][0]] = 0.0000001\n",
    "                counter[res_in[k][0]] += res_in[k][1]\n",
    "                total_score += res_in[k][1]\n",
    "            ent = 0.0000001\n",
    "            scores,ents, ent_dic = [], [], {}\n",
    "            for key in counter:\n",
    "                scores.append(counter[key] / total_score)\n",
    "                p = counter[key] / total_score\n",
    "                logp = np.log2(p)\n",
    "                ent -= p * logp\n",
    "            ents.append(ent)            \n",
    "            ent_dict[i] = ent\n",
    "        sort_dic = sorted(ent_dict.items(), key = lambda item:item[1], reverse = True)[:self.k]\n",
    "        cur_set_index = [sort_dic[k][0] for k in range(len(sort_dic))]\n",
    "        return cur_set_index\n",
    "            \n",
    "    def merge(self, cur_set):\n",
    "        self.sets = [] \n",
    "        new_labeled = pd.concat((self.labeled, self.unlabeled.loc[cur_set]), axis = 0)\n",
    "        self.unlabeled.loc[cur_set,'text'].to_csv('../data/new_labeled' + str(len(new_labeled)) + '.csv')        \n",
    "        new_unlabeled = self.unlabeled.drop(cur_set, axis = 0)\n",
    "        self.labeled = new_labeled\n",
    "        self.unlabeled = new_unlabeled\n",
    "        self.labeled_texts = list(self.labeled['text'])\n",
    "        self.unlabeled_texts = list(self.unlabeled['text'])\n",
    "        self.unlabeled_indexes = new_unlabeled.index.tolist()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104983 11116 116099\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAE+BJREFUeJzt3W+MXfV95/H3p3ZoaLrEJkwt1oa1\nV7ESUaQEYoGjVFUWdsGQqOZBSonaYiEaPwjZTVetWqdPUJMiEWlVGrQpEgoupsqGIJouVuLEazlU\nbR9AMCULARIxS8JiC7Br86db1LCk331wf5NeZmeYufOzffHc90u6uud8z++c3+/4jOfj8+dep6qQ\nJKnHz4x7AJKkU59hIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSp28pxD+BkOeus\ns2r9+vXjHoYknVIefvjhv6+qqYXaTUyYrF+/ngMHDox7GJJ0SknyzGLaeZlLktTNMJEkdTNMJEnd\nDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1G1iPgEvjcP6Hd/46fSPbv7IGEcinViemUiSuhkm\nkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkm\nkqRuiwqTJKuS3Jvk+0meTPLBJGcm2Zfkqfa+urVNkluTTCd5NMmFQ9vZ1to/lWTbUP0DSR5r69ya\nJK0+ch+SpJNvsWcmXwC+VVXvBd4HPAnsAPZX1UZgf5sHuALY2F7bgdtgEAzAjcDFwEXAjTPh0Np8\nYmi9La0+Uh+SpPFYMEySvBP4ZeAOgKp6rapeArYCu1qzXcBVbXorcFcNPACsSnI2cDmwr6qOVdWL\nwD5gS1t2RlU9UFUF3DVrW6P0IUkag8WcmWwAjgB/luSRJF9K8g5gTVU919o8D6xp02uBZ4fWP9hq\nb1Y/OEedJfQhSRqDxYTJSuBC4LaqugD4R/7lchMA7Yyijv/w+vpIsj3JgSQHjhw5coJGJklaTJgc\nBA5W1YNt/l4G4fLCzKWl9n64LT8EnDO0/rpWe7P6ujnqLKGPN6iq26tqU1VtmpqaWsSuSpKWYsEw\nqarngWeTvKeVLgWeAHYDM09kbQPua9O7gWvbE1ebgZfbpaq9wGVJVrcb75cBe9uyV5Jsbk9xXTtr\nW6P0IUkag5WLbPcfgS8nOQ14GriOQRDdk+R64Bng6tZ2D3AlMA282tpSVceSfA54qLX7bFUda9Of\nBO4ETge+2V4AN4/ShyRpPBYVJlX1XWDTHIsunaNtATfMs52dwM456geA8+eoHx21D0nSyecn4CVJ\n3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ\n3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUrdFhUmSHyV5LMl3kxxo\ntTOT7EvyVHtf3epJcmuS6SSPJrlwaDvbWvunkmwbqn+gbX+6rZul9iFJOvlGOTP5d1X1/qra1OZ3\nAPuraiOwv80DXAFsbK/twG0wCAbgRuBi4CLgxplwaG0+MbTelqX0IUkaj57LXFuBXW16F3DVUP2u\nGngAWJXkbOByYF9VHauqF4F9wJa27IyqeqCqCrhr1rZG6UOSNAaLDZMC/keSh5Nsb7U1VfVcm34e\nWNOm1wLPDq17sNXerH5wjvpS+pAkjcHKRbb7pao6lOQXgH1Jvj+8sKoqSR3/4fX10YJvO8C55557\nQsYlSVrkmUlVHWrvh4G/ZHDP44WZS0vt/XBrfgg4Z2j1da32ZvV1c9RZQh+zx317VW2qqk1TU1OL\n2VVJ0hIsGCZJ3pHkX81MA5cB3wN2AzNPZG0D7mvTu4Fr2xNXm4GX26WqvcBlSVa3G++XAXvbsleS\nbG5PcV07a1uj9CEtK+t3fOOnL+mtbDGXudYAf9me1l0J/Leq+laSh4B7klwPPANc3drvAa4EpoFX\ngesAqupYks8BD7V2n62qY236k8CdwOnAN9sL4OZR+pAkjceCYVJVTwPvm6N+FLh0jnoBN8yzrZ3A\nzjnqB4Dzj0cfkqSTz0/AS5K6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZ\nJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZ\nJpKkboaJJKnbosMkyYokjyT5epvfkOTBJNNJvprktFb/2TY/3ZavH9rGZ1r9B0kuH6pvabXpJDuG\n6iP3IUk6+UY5M/k08OTQ/OeBW6rq3cCLwPWtfj3wYqvf0tqR5DzgGuAXgS3An7aAWgF8EbgCOA/4\neGs7ch+SpPFYVJgkWQd8BPhSmw9wCXBva7ILuKpNb23ztOWXtvZbgbur6sdV9UNgGriovaar6umq\neg24G9i6xD4kSWOw2DOTPwF+D/jnNv8u4KWqer3NHwTWtum1wLMAbfnLrf1P67PWma++lD7eIMn2\nJAeSHDhy5Mgid1WSNKoFwyTJR4HDVfXwSRjPcVVVt1fVpqraNDU1Ne7hSNKytXIRbT4E/EqSK4G3\nA2cAXwBWJVnZzgzWAYda+0PAOcDBJCuBdwJHh+ozhteZq350CX1IksZgwTOTqvpMVa2rqvUMbqB/\nu6p+Hbgf+Fhrtg24r03vbvO05d+uqmr1a9qTWBuAjcB3gIeAje3JrdNaH7vbOqP2IUkag8Wcmczn\n94G7k/wR8AhwR6vfAfx5kmngGINwoKoeT3IP8ATwOnBDVf0EIMmngL3ACmBnVT2+lD4kSeMxUphU\n1V8Bf9Wmn2bwJNbsNv8E/Oo8698E3DRHfQ+wZ476yH1Ikk4+PwEvSepmmEiSuhkmkqRuhokkqZth\nIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZth\nIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6LRgmSd6e5DtJ/meSx5P8YatvSPJgkukkX01yWqv/\nbJufbsvXD23rM63+gySXD9W3tNp0kh1D9ZH7kCSdfIs5M/kxcElVvQ94P7AlyWbg88AtVfVu4EXg\n+tb+euDFVr+ltSPJecA1wC8CW4A/TbIiyQrgi8AVwHnAx1tbRu1DkjQeC4ZJDfyfNvu29irgEuDe\nVt8FXNWmt7Z52vJLk6TV766qH1fVD4Fp4KL2mq6qp6vqNeBuYGtbZ9Q+JEljsKh7Ju0M4rvAYWAf\n8L+Al6rq9dbkILC2Ta8FngVoy18G3jVcn7XOfPV3LaEPSdIYLCpMquonVfV+YB2DM4n3ntBRHSdJ\ntic5kOTAkSNHxj0cSVq2Rnqaq6peAu4HPgisSrKyLVoHHGrTh4BzANrydwJHh+uz1pmvfnQJfcwe\n7+1VtamqNk1NTY2yq5KkESzmaa6pJKva9OnAfwCeZBAqH2vNtgH3tendbZ62/NtVVa1+TXsSawOw\nEfgO8BCwsT25dRqDm/S72zqj9iFJGoOVCzfhbGBXe+rqZ4B7qurrSZ4A7k7yR8AjwB2t/R3AnyeZ\nBo4xCAeq6vEk9wBPAK8DN1TVTwCSfArYC6wAdlbV421bvz9KH5Kk8VgwTKrqUeCCOepPM7h/Mrv+\nT8CvzrOtm4Cb5qjvAfYcjz4kSSefn4CXJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NE\nktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NE\nktTNMJEkdTNMJEndFgyTJOckuT/JE0keT/LpVj8zyb4kT7X31a2eJLcmmU7yaJILh7a1rbV/Ksm2\nofoHkjzW1rk1SZbahyTp5FvMmcnrwO9U1XnAZuCGJOcBO4D9VbUR2N/mAa4ANrbXduA2GAQDcCNw\nMXARcONMOLQ2nxhab0urj9SHJGk8FgyTqnquqv6uTf8D8CSwFtgK7GrNdgFXtemtwF018ACwKsnZ\nwOXAvqo6VlUvAvuALW3ZGVX1QFUVcNesbY3ShyRpDEa6Z5JkPXAB8CCwpqqea4ueB9a06bXAs0Or\nHWy1N6sfnKPOEvqQJI3BosMkyc8DfwH8dlW9MrysnVHUcR7bGyyljyTbkxxIcuDIkSMnaGSSpEWF\nSZK3MQiSL1fV11r5hZlLS+39cKsfAs4ZWn1dq71Zfd0c9aX08QZVdXtVbaqqTVNTU4vZVUnSEizm\naa4AdwBPVtUfDy3aDcw8kbUNuG+ofm174moz8HK7VLUXuCzJ6nbj/TJgb1v2SpLNra9rZ21rlD4k\nSWOwchFtPgT8JvBYku+22h8ANwP3JLkeeAa4ui3bA1wJTAOvAtcBVNWxJJ8DHmrtPltVx9r0J4E7\ngdOBb7YXo/YhSRqPBcOkqv4WyDyLL52jfQE3zLOtncDOOeoHgPPnqB8dtQ9J0snnJ+AlSd0ME0lS\nN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lS\nN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1G3BMEmyM8nhJN8bqp2Z\nZF+Sp9r76lZPkluTTCd5NMmFQ+tsa+2fSrJtqP6BJI+1dW5NkqX2IUkaj8WcmdwJbJlV2wHsr6qN\nwP42D3AFsLG9tgO3wSAYgBuBi4GLgBtnwqG1+cTQeluW0ockaXwWDJOq+mvg2KzyVmBXm94FXDVU\nv6sGHgBWJTkbuBzYV1XHqupFYB+wpS07o6oeqKoC7pq1rVH6kCSNycolrremqp5r088Da9r0WuDZ\noXYHW+3N6gfnqC+lj+eYJcl2BmcvnHvuuYvcNWn5WL/jGz+d/tHNHxnjSLTcdd+Ab2cUdRzGctz7\nqKrbq2pTVW2ampo6ASOTJMHSw+SFmUtL7f1wqx8Czhlqt67V3qy+bo76UvqQJI3JUsNkNzDzRNY2\n4L6h+rXtiavNwMvtUtVe4LIkq9uN98uAvW3ZK0k2t6e4rp21rVH6kCSNyYL3TJJ8BfgwcFaSgwye\nyroZuCfJ9cAzwNWt+R7gSmAaeBW4DqCqjiX5HPBQa/fZqpq5qf9JBk+MnQ58s70YtQ9J0vgsGCZV\n9fF5Fl06R9sCbphnOzuBnXPUDwDnz1E/OmofkqTx8BPwkqRuhokkqZthIknqZphIkroZJpKkboaJ\nJKmbYSJJ6rbUL3qUJoJflCgtjmcmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6\nGSaSpG6GiSSpm2EiSepmmEia1/od33jD95NJ8zFMJEndDBNJUrdTNkySbEnygyTTSXaMezySNMlO\nyTBJsgL4InAFcB7w8STnjXdUkjS5TskwAS4Cpqvq6ap6Dbgb2DrmMUmaxRv4k+NUDZO1wLND8wdb\nTdIyYhidOlJV4x7DyJJ8DNhSVb/V5n8TuLiqPjWr3XZge5t9D3AU+PuTOda3kLOY3H2Hyd7/Sd53\nmOz9Px77/m+qamqhRqfq/wF/CDhnaH5dq71BVd0O3D4zn+RAVW068cN765nkfYfJ3v9J3neY7P0/\nmft+ql7megjYmGRDktOAa4DdYx6TJE2sU/LMpKpeT/IpYC+wAthZVY+PeViSNLFOyTABqKo9wJ4R\nV7t94SbL1iTvO0z2/k/yvsNk7/9J2/dT8ga8JOmt5VS9ZyJJeguZiDCZtK9eSXJOkvuTPJHk8SSf\nbvUzk+xL8lR7Xz3usZ4oSVYkeSTJ19v8hiQPtp+Br7YHN5alJKuS3Jvk+0meTPLBSTn2Sf5z+5n/\nXpKvJHn7cj72SXYmOZzke0O1OY91Bm5tfw6PJrnweI5l2YfJhH71yuvA71TVecBm4Ia2zzuA/VW1\nEdjf5perTwNPDs1/Hrilqt4NvAhcP5ZRnRxfAL5VVe8F3sfgz2HZH/ska4H/BGyqqvMZPJxzDcv7\n2N8JbJlVm+9YXwFsbK/twG3HcyDLPkyYwK9eqarnqurv2vQ/MPhlspbBfu9qzXYBV41nhCdWknXA\nR4AvtfkAlwD3tibLed/fCfwycAdAVb1WVS8xIceewUNFpydZCfwc8BzL+NhX1V8Dx2aV5zvWW4G7\nauABYFWSs4/XWCYhTCb6q1eSrAcuAB4E1lTVc23R88CaMQ3rRPsT4PeAf27z7wJeqqrX2/xy/hnY\nABwB/qxd5vtSkncwAce+qg4B/wX43wxC5GXgYSbn2M+Y71if0N+FkxAmEyvJzwN/Afx2Vb0yvKwG\nj/Etu0f5knwUOFxVD497LGOyErgQuK2qLgD+kVmXtJbxsV/N4F/fG4B/DbyD//8S0EQ5mcd6EsJk\nUV+9stwkeRuDIPlyVX2tlV+YOa1t74fHNb4T6EPAryT5EYNLmpcwuIewql36gOX9M3AQOFhVD7b5\nexmEyyQc+38P/LCqjlTV/wW+xuDnYVKO/Yz5jvUJ/V04CWEycV+90u4R3AE8WVV/PLRoN7CtTW8D\n7jvZYzvRquozVbWuqtYzONbfrqpfB+4HPtaaLct9B6iq54Fnk7ynlS4FnmACjj2Dy1ubk/xc+zsw\ns+8TceyHzHesdwPXtqe6NgMvD10O6zYRH1pMciWD6+gzX71y05iHdEIl+SXgb4DH+Jf7Bn/A4L7J\nPcC5wDPA1VU1++bdspHkw8DvVtVHk/xbBmcqZwKPAL9RVT8e5/hOlCTvZ/DwwWnA08B1DP7huOyP\nfZI/BH6NwRONjwC/xeC+wLI89km+AnyYwbcDvwDcCPx35jjWLWD/K4NLf68C11XVgeM2lkkIE0nS\niTUJl7kkSSeYYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRu/w8IImwt94EEmwAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#if __name__==\"__main__\":\n",
    "data_util = data('../data/data.txt', '../data/text.csv', '../data/knowledge.txt', '../data/label.txt')\n",
    "word_util = word()\n",
    "word_util.get_word2vec()\n",
    "#labeled, unlabeled, test = data_util.split_labeled_unlabeld_and_test()\n",
    "labeled, unlabeled = data_util.split_real_labeled_unlabeld_and_test()\n",
    "test = None\n",
    "labeled_texts = labeled['text']\n",
    "#test_texts = test['text']\n",
    "dic = data_util.get_dict_of_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of cur unlabeled index is : 104983\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n"
     ]
    }
   ],
   "source": [
    "labeled_indexes, unlabeled_indexes = data_util.labeled_indexes, data_util.unlabeled_indexes\n",
    "al = active_learning(labeled, unlabeled, test, dic, unlabeled_indexes, 500, word_util)\n",
    "while len(al.unlabeled_indexes) > 500:\n",
    "    print 'length of cur unlabeled index is : ' + str(len(al.unlabeled_indexes))\n",
    "    #al.predict()\n",
    "    cur_set = al.select_set()\n",
    "    al.merge(cur_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfenv.v1.2]",
   "language": "python",
   "name": "conda-env-tfenv.v1.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
