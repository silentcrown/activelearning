{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/lil.py:19: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _csparsetools\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:165: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._shortest_path import shortest_path, floyd_warshall, dijkstra,\\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._tools import csgraph_to_dense, csgraph_from_dense,\\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:167: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._traversal import breadth_first_order, depth_first_order, \\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:169: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._min_spanning_tree import minimum_spanning_tree\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:170: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._reordering import reverse_cuthill_mckee, maximum_bipartite_matching, \\\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/special/__init__.py:640: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ufuncs import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/linalg/basic.py:17: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._solve_toeplitz import levinson\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/linalg/__init__.py:207: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._decomp_update import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/special/_ellip_harm.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/interpolate/_bsplines.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _bspl\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/__init__.py:95: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .ckdtree import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/__init__.py:96: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .qhull import *\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/_spherical_voronoi.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _voronoi\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/spatial/distance.py:122: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _hausdorff\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/optimize/_trlib/__init__.py:1: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._trlib import TRLIBQuadraticSubproblem\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/optimize/_numdiff.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._group_columns import group_dense, group_sparse\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/scipy/stats/_continuous_distns.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _stats\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/__init__.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/__init__.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/h5py/_hl/group.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .. import h5g, h5i, h5o, h5r, h5t, h5l, h5p\n",
      "Using TensorFlow backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.270 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/utils/__init__.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .murmurhash import murmurhash3_32\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/utils/extmath.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._logistic_sigmoid import _log_logistic_sigmoid\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/utils/extmath.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .sparsefuncs_fast import csr_row_norms\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/metrics/cluster/supervised.py:23: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .expected_mutual_info_fast import expected_mutual_information\n",
      "/home/works/data/tools/miniconda2/envs/tfenv.v1.2/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 1006 user-define jieba dict success!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from intent_classify import IntentClassify\n",
    "from gensim import corpora, models, similarities,matutils\n",
    "from fasttext_util import FasttextClassifier\n",
    "import time\n",
    "from collections import Counter\n",
    "import re\n",
    "from url import Searcher\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word():    \n",
    "    def __init__(self):\n",
    "        self.word_to_vec = {}\n",
    "        self.dictionary = corpora.dictionary.Dictionary()\n",
    "        self.dic = self.dictionary.load('../util/new/laiye/corpus.dict')\n",
    "        self.tfidf = models.TfidfModel.load('../util/new/laiye/corpus.tfidf_model')  \n",
    "        \n",
    "    def remove_punctuation(self, line):\n",
    "        rule = re.compile(ur\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")\n",
    "        line = rule.sub('',line)\n",
    "        return line        \n",
    "\n",
    "    def stopwordslist(self, filepath):  \n",
    "        stopwords = [line.strip() for line in open(filepath, 'r').readlines()]  \n",
    "        return stopwords  \n",
    "\n",
    "\n",
    "    # 对句子进行分词  \n",
    "    def jieba_cut(self, sentence):  \n",
    "        sentence_seged = jieba.cut(sentence.strip())  \n",
    "        stopwords = stopwordslist('../util/new/laiye/stopwords.txt')  # 这里加载停用词的路径  \n",
    "        outstr = ''  \n",
    "        for word in sentence_seged:  \n",
    "            if word.encode('utf-8') not in stopwords:  \n",
    "                if word.encode('utf-8') != '\\t' and word != '\\t':  \n",
    "                    outstr += word\n",
    "                    outstr += \" \"  \n",
    "        return outstr.strip().split(' ')\n",
    "\n",
    "    def get_word2vec(self):\n",
    "        with open('../util/new/laiye/w2v_sgns_win1_d80.kv') as f:\n",
    "            data = [x.split(' ') for x in f.readlines()[1:]]\n",
    "            words = [d[0] for d in data]\n",
    "            vecs = np.array([d[1: -1] for d in data], dtype= 'float64')\n",
    "            for i in range(len(words)):\n",
    "                word = words[i]\n",
    "                self.word_to_vec[word] = vecs[i]\n",
    "\n",
    "    def get_tf_idf_of_query(self, query, dic, tfidf):      \n",
    "        vec_bow = dic.doc2bow(query)\n",
    "        vec_tfidf = tfidf[vec_bow]\n",
    "        tp = [0.0] * len(query)\n",
    "        ids = [tid[0] for tid in vec_tfidf]\n",
    "        flags = [0] * len(query)\n",
    "        count = 0.00001\n",
    "        for j in range(len(query)):\n",
    "            for i in range(len(ids)):\n",
    "                if self.dic[ids[i]] == query[j]:\n",
    "                    tp[j] = vec_tfidf[i][1]\n",
    "                    flags[j] = 1\n",
    "                    count += 1\n",
    "                    break\n",
    "        sums = sum(tp)\n",
    "        #print ','.join(query)\n",
    "        for i in range(len(flags)):\n",
    "            if flags[i] == 1:\n",
    "                continue\n",
    "            tp[i] = 1.0 * sums / count\n",
    "                    \n",
    "        # apply l1-norm to tfidf value\n",
    "        #tfidf = matutils.unitvec(vec_tfidf, norm = 'l1')   \n",
    "        return tp\n",
    "\n",
    "    def get_vectors_of_data_cut(self, data):\n",
    "        vecs = []\n",
    "        for i in range(len(data)):\n",
    "            vec = []\n",
    "            d_line = data[i].split(' ')\n",
    "            tfidfs = self.get_tf_idf_of_query(d_line, self.dic, self.tfidf)\n",
    "            s = sum(tfidfs)\n",
    "            for j in range(len(d_line)):\n",
    "                if d_line[j].encode('utf-8') in self.word_to_vec:\n",
    "                    if s == 0:\n",
    "                        vec.append([0.0] * len(self.word_to_vec['家']))\n",
    "                    else:\n",
    "                        vec.append((tfidfs[j] / s) * self.word_to_vec[d_line[j].encode('utf-8')])\n",
    "            if len(vec) == 0:\n",
    "                vec.append([0.0] * len(self.word_to_vec['家']))\n",
    "            vecs.append(np.sum(np.array(vec), axis = 0))\n",
    "        return np.array(vecs)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data():\n",
    "    \n",
    "    def __init__(self, data_path, text_path, knowledge_path, label_path):\n",
    "        self.data = open(data_path, 'r').readlines()\n",
    "        self.knowledge = open(knowledge_path, 'r').readlines()\n",
    "        self.text_path = text_path\n",
    "        self.label = open(label_path, 'r').readlines()\n",
    "        self.indexes = []\n",
    "        self.labeled_indexes = []\n",
    "        self.unlabeled_indexes = []\n",
    "        self.test_indexes = []\n",
    "        self.questions_match_dict = []\n",
    "        self.t_map = {}\n",
    "        self.range = [0.000001,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0]\n",
    "        self.range_result = {0.05:0, 0.1:0,0.15:0, 0.2:0,0.25:0, 0.3:0,0.35:0, 0.4:0,0.45:0,0.5:0, 0.55:0,0.6:0,0.65:0,0.7:0, 0.75:0,0.8:0,0.85:0,0.9:0,0.95:0, 1.0:0}\n",
    "        self.dic = {}\n",
    "        \n",
    "    def get_texts(self, text_path):\n",
    "        texts = []\n",
    "        knowledges = []\n",
    "        labels = []\n",
    "        for i in range(len(self.data)):\n",
    "            for key in self.data[i].split('||'):\n",
    "                if key.strip() != '':\n",
    "                    key = key.strip().replace('\\n','')\n",
    "                    texts.append(key) \n",
    "                    knowledges.append(self.knowledge[i].strip().replace('\\n',''))\n",
    "                    labels.append(self.label[i].strip().replace('\\n',''))\n",
    "        text_csv = pd.DataFrame({'text':texts, 'knowledge':knowledges, 'label':labels})\n",
    "        text_csv.to_csv(text_path, index = False, encoding = 'utf-8')\n",
    "        \n",
    "    def split_real_labeled_unlabeld_and_test(self):\n",
    "        self.get_texts(self.text_path)\n",
    "        data = pd.read_csv(self.text_path)\n",
    "        l = list(data['text'])\n",
    "        k = list(data['knowledge'])\n",
    "        lab = list(data['label'])\n",
    "        '''\n",
    "        res = []\n",
    "        for i in range(len(l)):\n",
    "            res.append((l[i], i))\n",
    "        pickle.dump(res, open('total_texts.txt', 'w'))\n",
    "        '''\n",
    "        undefined = pd.read_csv('../data/undefined_text.csv')\n",
    "        labeled, unlabeled = data, undefined\n",
    "        return labeled, unlabeled\n",
    "    \n",
    "    def split_labeled_unlabeld_and_test(self):\n",
    "        self.get_texts(self.text_path)\n",
    "        data = pd.read_csv(self.text_path)\n",
    "        l = list(data['text'])\n",
    "        k = list(data['knowledge'])\n",
    "        lab = list(data['label'])\n",
    "        '''\n",
    "        res = []\n",
    "        for i in range(len(l)):\n",
    "            res.append((l[i], i))\n",
    "        pickle.dump(res, open('total_texts.txt', 'w'))\n",
    "        \n",
    "        import ipdb;\n",
    "        ipdb.set_trace()\n",
    "        '''\n",
    "        unique_knowledge = data['knowledge'].unique()\n",
    "        self.labeled_indexes = []\n",
    "        #data = data[data['label'] != '聊天']\n",
    "        for i in range(len(unique_knowledge)):\n",
    "            data_of_know = data[data['knowledge'] == unique_knowledge[i]]\n",
    "            inds = data_of_know.index.tolist()\n",
    "            random.shuffle(inds)\n",
    "            if len(inds) < 10:\n",
    "                self.labeled_indexes = self.labeled_indexes + inds[:int(0.5*len(inds)) + 1]\n",
    "            else:\n",
    "                self.labeled_indexes = self.labeled_indexes + inds[:10]\n",
    "            \n",
    "        self.indexes = data.index.tolist()\n",
    "        left_indexes = list(set(self.indexes) - set(self.labeled_indexes))\n",
    "        random.shuffle(left_indexes)\n",
    "        self.unlabeled_indexes, self.test_indexes = left_indexes[:int(0.8 * len(left_indexes))], left_indexes[int(0.8 * len(left_indexes)) :]\n",
    "        labeled, unlabeled, test = data.loc[self.labeled_indexes], data.loc[self.unlabeled_indexes], data.loc[self.test_indexes]\n",
    "        print len(self.unlabeled_indexes),len(self.test_indexes),len(self.labeled_indexes),len(self.indexes)\n",
    "        test.to_csv('../data/test_undefined.csv')\n",
    "        return labeled, unlabeled, test\n",
    "    \n",
    "    def trans_i_to_str_dic(self, dic):\n",
    "        i_to_str_map = pickle.load(open('total_texts.txt'))\n",
    "        for i in range(len(i_to_str_map)):\n",
    "            self.t_map[i_to_str_map[i][1]] = i_to_str_map[i][0]\n",
    "        new_dic = {}\n",
    "        for key in dic:\n",
    "            new_dic[self.t_map[key]] = dic[key]\n",
    "        return new_dic\n",
    "    \n",
    "    def get_dict_of_questions(self):\n",
    "        self.dic = pickle.load(open('../data/xiaolai100000+.txt', 'r'))#pickle.load(open('../data/total_dic_results.txt', 'r'))\n",
    "        for key in self.dic:\n",
    "            sim_result = self.dic[key]\n",
    "            scores = [sim_result[k][2] for k in range(len(sim_result))]\n",
    "            for m in range(len(scores)):\n",
    "                for j in range(len(self.range) - 1):\n",
    "                    if self.range[j] < float(scores[m]) <= self.range[j + 1]:\n",
    "                        self.range_result[self.range[j + 1]] += 1\n",
    "        xl = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0]\n",
    "        col_value = []\n",
    "        for i in range(len(xl)):\n",
    "            col_value.append(self.range_result[xl[i]])\n",
    "        plt.bar(range(5,105,5), col_value)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        def update_unlabeled_data(self, new_undefined_path):\n",
    "            origin_texts = [key for key in self.dic]\n",
    "            cur_undefined = pd.read_csv(new_undefined_path)\n",
    "            cur_texts = list(new_undefined['text'])\n",
    "            new_texts = list(set(cur_texts) - set(origin_texts))\n",
    "            return new_texts\n",
    "        \n",
    "        return self.dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class active_learning():\n",
    "    \n",
    "    def __init__(self, labeled, unlabeled, test, dic,  k, word_util, map_):\n",
    "        self.labeled = labeled\n",
    "        self.unlabeled = unlabeled\n",
    "        self.test = test\n",
    "        self.labeled_texts = list(labeled['text'])\n",
    "        self.unlabeled_texts = list(unlabeled['text'])\n",
    "        #self.test_texts = list(test['text'])\n",
    "        self.dic = dic\n",
    "        #self.real = list(test['knowledge'])\n",
    "        self.unlabeled_indexes = unlabeled.index.tolist()\n",
    "        self.k = k\n",
    "        self.sets = []\n",
    "        self.map = map_\n",
    "        self.word_util = word_util\n",
    "        \n",
    "    def predict(self):\n",
    "        pred = []\n",
    "        print 'now labeled sets size is : ' + str(len(self.labeled_texts))\n",
    "        for i in self.test.index.tolist():\n",
    "            results = self.dic[i]\n",
    "            res_in = []\n",
    "            for res in results:\n",
    "                ques, title, score = res\n",
    "                if ques.encode('utf-8') in self.labeled_texts:\n",
    "                    knowledge = self.labeled[self.labeled['text'] == ques.encode('utf-8')]['knowledge']\n",
    "                    knowledge = knowledge.loc[knowledge.index.tolist()[0]]\n",
    "                    res_in.append(knowledge)\n",
    "            if len(res_in) == 0:\n",
    "                res_in.append('聊天')\n",
    "            sort_dic = sorted(Counter(res_in).items(), key = lambda item:item[1], reverse = True)\n",
    "            final_pred = sort_dic[0][0]\n",
    "            pred.append(final_pred)\n",
    "        real = [self.real[i].decode('utf-8') for i in range(len(self.real))]\n",
    "        preds = [pred[i].decode('utf-8') for i in range(len(pred))]\n",
    "        print('classification_report :\\n%s' % metrics.classification_report(real, preds)) \n",
    "        \n",
    "    def select_set(self):\n",
    "        ent_dict = {}\n",
    "        count_dict = []\n",
    "        for i in self.unlabeled_indexes[:100001]:\n",
    "            if self.map[i] not in dic:\n",
    "                print i, self.map[i]\n",
    "                break\n",
    "            results = dic[self.map[i]]\n",
    "            res_in = []\n",
    "            if i % 100 == 0:\n",
    "                print i\n",
    "            for res in results:\n",
    "                ques, title, score = res\n",
    "                if ques.encode('utf-8') in self.labeled_texts and 0.45 <= float(score):\n",
    "                    knowledge = self.labeled[self.labeled['text'] == ques.encode('utf-8')]['knowledge']\n",
    "                    knowledge = knowledge.loc[knowledge.index.tolist()[0]]\n",
    "                    res_in.append((knowledge, float(score)))\n",
    "            if len(res_in) == 0:\n",
    "                res_in.append(('聊天',0.0000001))\n",
    "            counter = {}\n",
    "            total_score = 0.0000001\n",
    "            for k in range(len(res_in)):\n",
    "                if res_in[k][0] not in counter:\n",
    "                    counter[res_in[k][0]] = 0.0000001\n",
    "                counter[res_in[k][0]] += res_in[k][1]\n",
    "                total_score += res_in[k][1]\n",
    "            ent = 0.0000001\n",
    "            scores,ents, ent_dic = [], [], {}\n",
    "            for key in counter:\n",
    "                scores.append(counter[key] / total_score)\n",
    "                p = counter[key] / total_score\n",
    "                logp = np.log2(p)\n",
    "                ent -= p * logp\n",
    "            ents.append(ent)     \n",
    "            ent_dict[i] = ent\n",
    "            count_dict.append(counter)\n",
    "        sort_dic = sorted(ent_dict.items(), key = lambda item:item[1], reverse = True)[:self.k]\n",
    "        cur_set_index = [sort_dic[k][0] for k in range(len(sort_dic))]\n",
    "        cur_set_entropy = [sort_dic[k][1] for k in range(len(sort_dic))]\n",
    "        cur_count = []\n",
    "        for i in range(len(count_dict)):\n",
    "            if i in cur_set_index:\n",
    "                cur_count.append(count_dict[i])\n",
    "        \n",
    "        return cur_set_index, cur_set_entropy, cur_count\n",
    "            \n",
    "    def merge(self, cur_set, cur_set_entropy, cur_count):\n",
    "        self.sets = [] \n",
    "        new_labeled = pd.concat((self.labeled, self.unlabeled.loc[cur_set]), axis = 0)\n",
    "        dt = pd.DataFrame({'text':[], 'entropy':[], 'index':[], 'count' :[]})\n",
    "        print 'merge into file : '+ '../data/total_results/entropy_sum' + str(len(new_labeled)) + '.csv'\n",
    "        dt['text'] = list(self.unlabeled.loc[cur_set,'text'])\n",
    "        dt['count'] = cur_count\n",
    "        dt['entropy'] = cur_set_entropy\n",
    "        dt['index'] = self.unlabeled.loc[cur_set].index.tolist()\n",
    "        dt.to_csv('../data/total_results/entropy_sum' + str(len(new_labeled)) + '.csv', index = False, encoding = 'utf-8')   \n",
    "        new_unlabeled = self.unlabeled.drop(cur_set, axis = 0)\n",
    "        self.labeled = new_labeled\n",
    "        self.unlabeled = new_unlabeled\n",
    "        self.labeled_texts = list(self.labeled['text'])\n",
    "        self.unlabeled_texts = list(self.unlabeled['text'])\n",
    "        self.unlabeled_indexes = new_unlabeled.index.tolist()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if __name__==\"__main__\":\n",
    "data_util = data('../data/data.txt', '../data/text.csv', '../data/knowledge.txt', '../data/label.txt')\n",
    "word_util = word()\n",
    "word_util.get_word2vec()\n",
    "#labeled, unlabeled, test = data_util.split_labeled_unlabeld_and_test()\n",
    "labeled, unlabeled = data_util.split_real_labeled_unlabeld_and_test()\n",
    "test = None\n",
    "labeled_texts = labeled['text']\n",
    "#test_texts = test['text']\n",
    "#dic = data_util.get_dict_of_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFbNJREFUeJzt3X+MXeV95/H3pzi0NN3EJpm1WBvW\nrGIlokghYIGjVFUWtsaQKOaPlCXq1hai8Uohu8mqq9bpP1ZJIxFp1TRoUyQUXOwqG4JosliJE6/l\nUHX3DxNMyEKARJ4SWGwBdjE/ukFNlvS7f9xnkst07Jk7jz0Xz7xf0tU953uec57n+Iz98flx76Sq\nkCSpxy+NewCSpDOfYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqduycQ9gobz9\n7W+vNWvWjHsYknRGeeihh/6uqiZma7dkwmTNmjUcPHhw3MOQpDNKkqfn0s7LXJKkboaJJKmbYSJJ\n6maYSJK6GSaSpG6zhkmSdyb53tDrlSSfTHJukn1JDrX3Fa19ktyWZDLJI0kuHdrWltb+UJItQ/XL\nkjza1rktSVp95D4kSQtv1jCpqh9W1SVVdQlwGfAq8DVgG7C/qtYC+9s8wDXA2vbaCtwOg2AAtgNX\nAJcD26fCobX56NB6G1t9pD4kSeMx6mWuq4C/raqngU3AzlbfCVzXpjcBu2rgALA8yXnA1cC+qjpe\nVS8C+4CNbdlbqupADX6H8K5p2xqlD0nSGIwaJjcAX27TK6vq2Tb9HLCyTa8Cnhla53Crnax+eIb6\nfPqQJI3BnD8Bn+Rs4EPAp6Yvq6pKUqdyYKeijyRbGVwG44ILLjgt45JOZs22b/x8+qlbPzDGkUin\n1yhnJtcA362q59v881OXltr70VY/Apw/tN7qVjtZffUM9fn08TpVdUdVrauqdRMTs361jCRpnkYJ\nk4/wi0tcALuBqSeytgD3DdU3tyeu1gMvt0tVe4ENSVa0G+8bgL1t2StJ1renuDZP29YofUiSxmBO\nl7mSvBn4LeDfD5VvBe5JchPwNHB9q+8BrgUmGTz5dSNAVR1P8mngwdbulqo63qY/BtwFnAN8s71G\n7kOSNB5zCpOq+jHwtmm1Fxg83TW9bQE3n2A7O4AdM9QPAhfPUB+5D0nSwvMT8JKkboaJJKmbYSJJ\n6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqducv4Je0sLzK+x1pvDMRJLUzTCRJHUz\nTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdZtTmCRZnuTeJD9I8kSS9yY5N8m+\nJIfa+4rWNkluSzKZ5JEklw5tZ0trfyjJlqH6ZUkebevcliStPnIfkqSFN9czk88D36qqdwHvBp4A\ntgH7q2otsL/NA1wDrG2vrcDtMAgGYDtwBXA5sH0qHFqbjw6tt7HVR+pDkjQes4ZJkrcCvwncCVBV\nP62ql4BNwM7WbCdwXZveBOyqgQPA8iTnAVcD+6rqeFW9COwDNrZlb6mqA1VVwK5p2xqlD0nSGMzl\nzORC4BjwF0keTvLFJG8GVlbVs63Nc8DKNr0KeGZo/cOtdrL64RnqzKMPSdIYzCVMlgGXArdX1XuA\nH/OLy00AtDOKOvXD6+sjydYkB5McPHbs2GkamSRpLmFyGDhcVQ+0+XsZhMvzU5eW2vvRtvwIcP7Q\n+qtb7WT11TPUmUcfr1NVd1TVuqpaNzExMYddlSTNx6xhUlXPAc8keWcrXQU8DuwGpp7I2gLc16Z3\nA5vbE1frgZfbpaq9wIYkK9qN9w3A3rbslSTr21Ncm6dta5Q+JEljMNfftPgfgC8lORt4EriRQRDd\nk+Qm4Gng+tZ2D3AtMAm82tpSVceTfBp4sLW7paqOt+mPAXcB5wDfbC+AW0fpQ5I0HnMKk6r6HrBu\nhkVXzdC2gJtPsJ0dwI4Z6geBi2eovzBqH5Kkhecn4CVJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lS\nN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lS\nN8NEktTNMJEkdZtTmCR5KsmjSb6X5GCrnZtkX5JD7X1FqyfJbUkmkzyS5NKh7Wxp7Q8l2TJUv6xt\nf7Ktm/n2IUlaeKOcmfzrqrqkqta1+W3A/qpaC+xv8wDXAGvbaytwOwyCAdgOXAFcDmyfCofW5qND\n622cTx+SpPHoucy1CdjZpncC1w3Vd9XAAWB5kvOAq4F9VXW8ql4E9gEb27K3VNWBqipg17RtjdKH\nJGkM5homBfyPJA8l2dpqK6vq2Tb9HLCyTa8Cnhla93Crnax+eIb6fPqQJI3Bsjm2+42qOpLknwP7\nkvxgeGFVVZI69cPr66MF31aACy644LSMS5I0xzOTqjrS3o8CX2Nwz+P5qUtL7f1oa34EOH9o9dWt\ndrL66hnqzKOP6eO+o6rWVdW6iYmJueyqJGkeZg2TJG9O8s+mpoENwPeB3cDUE1lbgPva9G5gc3vi\naj3wcrtUtRfYkGRFu/G+Adjblr2SZH17imvztG2N0ockaQzmcplrJfC19rTuMuC/VdW3kjwI3JPk\nJuBp4PrWfg9wLTAJvArcCFBVx5N8Gniwtbulqo636Y8BdwHnAN9sL4BbR+lDkjQes4ZJVT0JvHuG\n+gvAVTPUC7j5BNvaAeyYoX4QuPhU9CFJWnh+Al6S1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNM\nJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNM\nJEndDBNJUjfDRJLUbc5hkuSsJA8n+XqbvzDJA0kmk3wlydmt/sttfrItXzO0jU+1+g+TXD1U39hq\nk0m2DdVH7kOStPBGOTP5BPDE0Pxngc9V1TuAF4GbWv0m4MVW/1xrR5KLgBuAXwc2An/eAuos4AvA\nNcBFwEda25H7kCSNx5zCJMlq4APAF9t8gCuBe1uTncB1bXpTm6ctv6q13wTcXVU/qaofAZPA5e01\nWVVPVtVPgbuBTfPsQ5I0BnM9M/kz4A+Af2zzbwNeqqrX2vxhYFWbXgU8A9CWv9za/7w+bZ0T1efT\nhyRpDGYNkyQfBI5W1UMLMJ5TKsnWJAeTHDx27Ni4hyNJi9ZczkzeB3woyVMMLkFdCXweWJ5kWWuz\nGjjSpo8A5wO05W8FXhiuT1vnRPUX5tHH61TVHVW1rqrWTUxMzGFXJUnzMWuYVNWnqmp1Va1hcAP9\n21X1O8D9wIdbsy3AfW16d5unLf92VVWr39CexLoQWAt8B3gQWNue3Dq79bG7rTNqH5KkMVg2e5MT\n+kPg7iR/AjwM3NnqdwJ/mWQSOM4gHKiqx5LcAzwOvAbcXFU/A0jycWAvcBawo6oem08fkqTxGClM\nquqvgb9u008yeBJrept/AH77BOt/BvjMDPU9wJ4Z6iP3IUlaeH4CXpLUzTCRJHUzTCRJ3QwTSVI3\nw0SS1M0wkSR1M0wkSd0ME0lSN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3\nw0SS1M0wkSR1M0wkSd0ME0lSN8NEktRt1jBJ8itJvpPkfyd5LMkft/qFSR5IMpnkK0nObvVfbvOT\nbfmaoW19qtV/mOTqofrGVptMsm2oPnIfkqSFN5czk58AV1bVu4FLgI1J1gOfBT5XVe8AXgRuau1v\nAl5s9c+1diS5CLgB+HVgI/DnSc5KchbwBeAa4CLgI60to/YhSRqPWcOkBv5vm31TexVwJXBvq+8E\nrmvTm9o8bflVSdLqd1fVT6rqR8AkcHl7TVbVk1X1U+BuYFNbZ9Q+JEljMKd7Ju0M4nvAUWAf8LfA\nS1X1WmtyGFjVplcBzwC05S8DbxuuT1vnRPW3zaOP6ePemuRgkoPHjh2by65KkuZhTmFSVT+rqkuA\n1QzOJN51Wkd1ilTVHVW1rqrWTUxMjHs4krRojfQ0V1W9BNwPvBdYnmRZW7QaONKmjwDnA7TlbwVe\nGK5PW+dE9Rfm0YckaQzm8jTXRJLlbfoc4LeAJxiEyodbsy3AfW16d5unLf92VVWr39CexLoQWAt8\nB3gQWNue3DqbwU363W2dUfuQJI3BstmbcB6wsz119UvAPVX19SSPA3cn+RPgYeDO1v5O4C+TTALH\nGYQDVfVYknuAx4HXgJur6mcAST4O7AXOAnZU1WNtW384Sh+SpPGYNUyq6hHgPTPUn2Rw/2R6/R+A\n3z7Btj4DfGaG+h5gz6noQ5K08PwEvCSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKk\nbnP5BLykM9Sabd/4+fRTt35gjCPRYueZiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZ\nJpKkboaJJKmbYSJJ6maYSJK6zRomSc5Pcn+Sx5M8luQTrX5ukn1JDrX3Fa2eJLclmUzySJJLh7a1\npbU/lGTLUP2yJI+2dW5Lkvn2IUlaeHM5M3kN+P2qughYD9yc5CJgG7C/qtYC+9s8wDXA2vbaCtwO\ng2AAtgNXAJcD26fCobX56NB6G1t9pD4kSeMxa5hU1bNV9d02/ffAE8AqYBOwszXbCVzXpjcBu2rg\nALA8yXnA1cC+qjpeVS8C+4CNbdlbqupAVRWwa9q2RulDkjQGI90zSbIGeA/wALCyqp5ti54DVrbp\nVcAzQ6sdbrWT1Q/PUGcefUiSxmDOYZLk14C/Aj5ZVa8ML2tnFHWKx/Y68+kjydYkB5McPHbs2Gka\nmSRpTmGS5E0MguRLVfXVVn5+6tJSez/a6keA84dWX91qJ6uvnqE+nz5ep6ruqKp1VbVuYmJiLrsq\nSZqHuTzNFeBO4Imq+tOhRbuBqSeytgD3DdU3tyeu1gMvt0tVe4ENSVa0G+8bgL1t2StJ1re+Nk/b\n1ih9SJLGYC6/tvd9wO8Cjyb5Xqv9EXArcE+Sm4Cngevbsj3AtcAk8CpwI0BVHU/yaeDB1u6Wqjre\npj8G3AWcA3yzvRi1D0nSeMwaJlX1v4CcYPFVM7Qv4OYTbGsHsGOG+kHg4hnqL4zahyRp4fkJeElS\nN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR1M0wkSd0ME0lS\nN8NEktTNMJEkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVK3WcMkyY4kR5N8f6h2bpJ9SQ61\n9xWtniS3JZlM8kiSS4fW2dLaH0qyZah+WZJH2zq3Jcl8+5AkjcdczkzuAjZOq20D9lfVWmB/mwe4\nBljbXluB22EQDMB24ArgcmD7VDi0Nh8dWm/jfPqQJI3PrGFSVX8DHJ9W3gTsbNM7geuG6rtq4ACw\nPMl5wNXAvqo6XlUvAvuAjW3ZW6rqQFUVsGvatkbpQ5I0JvO9Z7Kyqp5t088BK9v0KuCZoXaHW+1k\n9cMz1OfTxz+RZGuSg0kOHjt2bI67JkkaVfcN+HZGUadgLKe8j6q6o6rWVdW6iYmJ0zAySRLMP0ye\nn7q01N6PtvoR4Pyhdqtb7WT11TPU59OHJGlM5hsmu4GpJ7K2APcN1Te3J67WAy+3S1V7gQ1JVrQb\n7xuAvW3ZK0nWt6e4Nk/b1ih9SJLGZNlsDZJ8GXg/8PYkhxk8lXUrcE+Sm4Cngetb8z3AtcAk8Cpw\nI0BVHU/yaeDB1u6Wqpq6qf8xBk+MnQN8s70YtQ9Jp96abd8A4KlbPzDmkeiNbtYwqaqPnGDRVTO0\nLeDmE2xnB7BjhvpB4OIZ6i+M2ockaTz8BLwkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6jbro8HS\nUjb1OQvwsxbSyXhmIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiaTTZs22b7zuszpa\nvAwTSVI3w0SS1M0wkSR1M0wkSd0ME0lStzM2TJJsTPLDJJNJto17PJK0lJ2RYZLkLOALwDXARcBH\nklw03lFJ0tJ1RoYJcDkwWVVPVtVPgbuBTWMek6RTzM+pnDnO1F+OtQp4Zmj+MHDFmMYi6Q1qKoiW\n4i82W+hf7JaqOu2dnGpJPgxsrKrfa/O/C1xRVR+f1m4rsLXNvhN4Afi7hRzrG8jbWbr7Dkt7/5fy\nvsPS3v9Tse//sqomZmt0pp6ZHAHOH5pf3WqvU1V3AHdMzSc5WFXrTv/w3niW8r7D0t7/pbzvsLT3\nfyH3/Uy9Z/IgsDbJhUnOBm4Ado95TJK0ZJ2RZyZV9VqSjwN7gbOAHVX12JiHJUlL1hkZJgBVtQfY\nM+Jqd8zeZNFayvsOS3v/l/K+w9Le/wXb9zPyBrwk6Y3lTL1nIkl6A1kSYbLUvnolyflJ7k/yeJLH\nknyi1c9Nsi/Jofa+YtxjPV2SnJXk4SRfb/MXJnmg/Qx8pT24sSglWZ7k3iQ/SPJEkvculWOf5D+1\nn/nvJ/lykl9ZzMc+yY4kR5N8f6g247HOwG3tz+GRJJeeyrEs+jBZol+98hrw+1V1EbAeuLnt8zZg\nf1WtBfa3+cXqE8ATQ/OfBT5XVe8AXgRuGsuoFsbngW9V1buAdzP4c1j0xz7JKuA/Auuq6mIGD+fc\nwOI+9ncBG6fVTnSsrwHWttdW4PZTOZBFHyYswa9eqapnq+q7bfrvGfxjsorBfu9szXYC141nhKdX\nktXAB4AvtvkAVwL3tiaLed/fCvwmcCdAVf20ql5iiRx7Bg8VnZNkGfCrwLMs4mNfVX8DHJ9WPtGx\n3gTsqoEDwPIk552qsSyFMJnpq1dWjWksCy7JGuA9wAPAyqp6ti16Dlg5pmGdbn8G/AHwj23+bcBL\nVfVam1/MPwMXAseAv2iX+b6Y5M0sgWNfVUeA/wL8HwYh8jLwEEvn2E850bE+rf8WLoUwWbKS/Brw\nV8Anq+qV4WU1eIxv0T3Kl+SDwNGqemjcYxmTZcClwO1V9R7gx0y7pLWIj/0KBv/7vhD4F8Cb+aeX\ngJaUhTzWSyFM5vTVK4tNkjcxCJIvVdVXW/n5qdPa9n50XOM7jd4HfCjJUwwuaV7J4B7C8nbpAxb3\nz8Bh4HBVPdDm72UQLkvh2P8b4EdVdayq/h/wVQY/D0vl2E850bE+rf8WLoUwWXJfvdLuEdwJPFFV\nfzq0aDewpU1vAe5b6LGdblX1qapaXVVrGBzrb1fV7wD3Ax9uzRblvgNU1XPAM0ne2UpXAY+zBI49\ng8tb65P8avs7MLXvS+LYDznRsd4NbG5Pda0HXh66HNZtSXxoMcm1DK6jT331ymfGPKTTKslvAP8T\neJRf3Df4Iwb3Te4BLgCeBq6vquk37xaNJO8H/nNVfTDJv2JwpnIu8DDw76rqJ+Mc3+mS5BIGDx+c\nDTwJ3MjgP46L/tgn+WPg3zJ4ovFh4PcY3BdYlMc+yZeB9zP4duDnge3Af2eGY90C9r8yuPT3KnBj\nVR08ZWNZCmEiSTq9lsJlLknSaWaYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqdv/B4b4\nVSSh0HDKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_dic = data_util.get_dict_of_questions()\n",
    "dic = data_util.trans_i_to_str_dic(t_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of cur unlabeled index is : 104983\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "49900\n",
      "50000\n",
      "50100\n",
      "50200\n",
      "50300\n",
      "50400\n",
      "50500\n",
      "50600\n",
      "50700\n",
      "50800\n",
      "50900\n",
      "51000\n",
      "51100\n",
      "51200\n",
      "51300\n",
      "51400\n",
      "51500\n",
      "51600\n",
      "51700\n",
      "51800\n",
      "51900\n",
      "52000\n",
      "52100\n",
      "52200\n",
      "52300\n",
      "52400\n",
      "52500\n",
      "52600\n",
      "52700\n",
      "52800\n",
      "52900\n",
      "53000\n",
      "53600\n",
      "53700\n",
      "53800\n",
      "53900\n",
      "54000\n",
      "54100\n",
      "54200\n",
      "54300\n",
      "54400\n",
      "54500\n",
      "54600\n",
      "54700\n",
      "54800\n",
      "54900\n",
      "55000\n",
      "55100\n",
      "55200\n",
      "55300\n",
      "55400\n",
      "55500\n",
      "55600\n",
      "55700\n",
      "55800\n",
      "55900\n",
      "56000\n",
      "56100\n",
      "56200\n",
      "56300\n",
      "56400\n",
      "56500\n",
      "56600\n",
      "56700\n",
      "56800\n",
      "56900\n",
      "57000\n",
      "57100\n",
      "57600\n",
      "57700\n",
      "57800\n",
      "57900\n",
      "58000\n",
      "58100\n",
      "58200\n",
      "58300\n",
      "58400\n",
      "58500\n",
      "58600\n",
      "58700\n",
      "58800\n",
      "58900\n",
      "59000\n",
      "59100\n",
      "59200\n",
      "59300\n",
      "59400\n",
      "59500\n",
      "59600\n",
      "59700\n",
      "59800\n",
      "59900\n",
      "60000\n",
      "60100\n",
      "60200\n",
      "60300\n",
      "60400\n",
      "60500\n",
      "60600\n",
      "60800\n",
      "60900\n",
      "61000\n",
      "61100\n",
      "61200\n",
      "61300\n",
      "61400\n",
      "61500\n",
      "61600\n",
      "61700\n",
      "61800\n",
      "61900\n",
      "62000\n",
      "62100\n",
      "62200\n",
      "62300\n",
      "62400\n",
      "62700\n",
      "62800\n",
      "62900\n",
      "63000\n",
      "63100\n",
      "63200\n",
      "63300\n",
      "63400\n",
      "63500\n",
      "63600\n",
      "63700\n",
      "63800\n",
      "63900\n",
      "64000\n",
      "64100\n",
      "64200\n",
      "64300\n",
      "64400\n",
      "64500\n",
      "64600\n",
      "64700\n",
      "64800\n",
      "64900\n",
      "65000\n",
      "65300\n",
      "65400\n",
      "65500\n",
      "65600\n",
      "65700\n",
      "65800\n",
      "65900\n",
      "66000\n",
      "66100\n",
      "66200\n",
      "66300\n",
      "66400\n",
      "66500\n",
      "66600\n",
      "66700\n",
      "66800\n",
      "66900\n",
      "67000\n",
      "67100\n",
      "67200\n",
      "67300\n",
      "67400\n",
      "67500\n",
      "67600\n",
      "67700\n",
      "67800\n",
      "67900\n",
      "68000\n",
      "68100\n",
      "68200\n",
      "68300\n",
      "68400\n",
      "68500\n",
      "69000\n",
      "69100\n",
      "69200\n",
      "69300\n",
      "69400\n",
      "69500\n",
      "69600\n",
      "69700\n",
      "69800\n",
      "69900\n",
      "70000\n",
      "70100\n",
      "70200\n",
      "70300\n",
      "70400\n",
      "70500\n",
      "70600\n",
      "70700\n",
      "70800\n",
      "70900\n",
      "71000\n",
      "71100\n",
      "71200\n",
      "71300\n",
      "71400\n",
      "71500\n",
      "71600\n",
      "71700\n",
      "71800\n",
      "71900\n",
      "72000\n",
      "72100\n",
      "72200\n",
      "72300\n",
      "72400\n",
      "72900\n",
      "73000\n",
      "73100\n",
      "73200\n",
      "73300\n",
      "73400\n",
      "73500\n",
      "73600\n",
      "73700\n",
      "73800\n",
      "73900\n",
      "74000\n",
      "74100\n",
      "74200\n",
      "74300\n",
      "74400\n",
      "74500\n",
      "74600\n",
      "74700\n",
      "74800\n",
      "74900\n",
      "75000\n",
      "75100\n",
      "75200\n",
      "75300\n",
      "75400\n",
      "75500\n",
      "75600\n",
      "75700\n",
      "75800\n",
      "75900\n",
      "76000\n",
      "76100\n",
      "76500\n",
      "76600\n",
      "76700\n",
      "76800\n",
      "76900\n",
      "77000\n",
      "77100\n",
      "77200\n",
      "77300\n",
      "77400\n",
      "77500\n",
      "77600\n",
      "77700\n",
      "77800\n",
      "77900\n",
      "78000\n",
      "78100\n",
      "78200\n",
      "78300\n",
      "78400\n",
      "78500\n",
      "78600\n",
      "78700\n",
      "78800\n",
      "78900\n",
      "79000\n",
      "79100\n",
      "79200\n",
      "79300\n",
      "79400\n",
      "79500\n",
      "79600\n",
      "79700\n",
      "79800\n",
      "79900\n",
      "80000\n",
      "80100\n",
      "80200\n",
      "80300\n",
      "80400\n",
      "80500\n",
      "80600\n",
      "80700\n",
      "81400\n",
      "81500\n",
      "81600\n",
      "81700\n",
      "81800\n",
      "81900\n",
      "82000\n",
      "82100\n",
      "82200\n",
      "82300\n",
      "82400\n",
      "82500\n",
      "82600\n",
      "82700\n",
      "82800\n",
      "82900\n",
      "83000\n",
      "83100\n",
      "83200\n",
      "83300\n",
      "83400\n",
      "83500\n",
      "83600\n",
      "83700\n",
      "83800\n",
      "83900\n",
      "84000\n",
      "84100\n",
      "84200\n",
      "84300\n",
      "84400\n",
      "84500\n",
      "84600\n",
      "84700\n",
      "84800\n",
      "84900\n",
      "85000\n",
      "85100\n",
      "85200\n",
      "85300\n",
      "85400\n",
      "85500\n",
      "85600\n",
      "85700\n",
      "85800\n",
      "85900\n",
      "86000\n",
      "86100\n",
      "86200\n",
      "86300\n",
      "86400\n",
      "86500\n",
      "86600\n",
      "86700\n",
      "87200\n",
      "87300\n",
      "87400\n",
      "87500\n",
      "87600\n",
      "87700\n",
      "87800\n",
      "87900\n",
      "88000\n",
      "88100\n",
      "88200\n",
      "88300\n",
      "88400\n",
      "88500\n",
      "88600\n",
      "88700\n",
      "88800\n",
      "88900\n",
      "89000\n",
      "89100\n",
      "89200\n",
      "89300\n",
      "89400\n",
      "89500\n",
      "89600\n",
      "89700\n",
      "89800\n",
      "89900\n",
      "90000\n",
      "90100\n",
      "90200\n",
      "90300\n",
      "90400\n",
      "90500\n",
      "90600\n",
      "90700\n",
      "90800\n",
      "90900\n",
      "91000\n",
      "91100\n",
      "91200\n",
      "91300\n",
      "91400\n",
      "91500\n",
      "91600\n",
      "91700\n",
      "91800\n",
      "91900\n",
      "92000\n",
      "92100\n",
      "92200\n",
      "92300\n",
      "92400\n",
      "92500\n",
      "92600\n",
      "92700\n",
      "92800\n",
      "92900\n",
      "93000\n",
      "93100\n",
      "93200\n",
      "93300\n",
      "93400\n",
      "93500\n",
      "93600\n",
      "93700\n",
      "93800\n",
      "93900\n",
      "94000\n",
      "94100\n",
      "94200\n",
      "94700\n",
      "94800\n",
      "94900\n",
      "95000\n",
      "95100\n",
      "95200\n",
      "95300\n",
      "95400\n",
      "95500\n",
      "95600\n",
      "95700\n",
      "95800\n",
      "95900\n",
      "96000\n",
      "96100\n",
      "96200\n",
      "96300\n",
      "96400\n",
      "96500\n",
      "96600\n",
      "96700\n",
      "96800\n",
      "96900\n",
      "97000\n",
      "97100\n",
      "97200\n",
      "97300\n",
      "97400\n",
      "97500\n",
      "97600\n",
      "97700\n",
      "97800\n",
      "97900\n",
      "98000\n",
      "98100\n",
      "98200\n",
      "98300\n",
      "98400\n",
      "98500\n",
      "98600\n",
      "98700\n",
      "98800\n",
      "98900\n",
      "99000\n",
      "99100\n",
      "99200\n",
      "99300\n",
      "99400\n",
      "99500\n",
      "99600\n",
      "99700\n",
      "99800\n",
      "99900\n",
      "100000\n",
      "merge into file : ../data/total_results/entropy_sum11616.csv\n"
     ]
    }
   ],
   "source": [
    "labeled_indexes, unlabeled_indexes = data_util.labeled_indexes, data_util.unlabeled_indexes\n",
    "al = active_learning(labeled, unlabeled, test, dic, 500, word_util, data_util.t_map)\n",
    "while len(al.unlabeled_indexes) > 500:\n",
    "    print 'length of cur unlabeled index is : ' + str(len(al.unlabeled_indexes))\n",
    "    #al.predict()\n",
    "    cur_set, cur_set_entropy, cur_count = al.select_set()\n",
    "    al.merge(cur_set, cur_set_entropy, cur_count)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfenv.v1.2]",
   "language": "python",
   "name": "conda-env-tfenv.v1.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
